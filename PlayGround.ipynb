{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DocumentProcessing.pdf_processing import pdf_processor\n",
    "from DocumentProcessing.pdf_processing import paper_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_paper = \"/root/gpt_projects/ABoringKnowledgeManagementSystem/DocumentProcessing/tests/1810.04805.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text, title,publish_display_data, authors, abstract, references = paper_processor.extract_text_from_pdf(pdf_paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = pdf_processor.extract_text(pdf_paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_processor.extract_toc_from_pdf(pdf_paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text_2[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_processor.extract_pdf_metadata(pdf_paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_processor.py\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path, max_non_text_pages=10):\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        text = ''\n",
    "        non_text_page_count = 0\n",
    "\n",
    "        for page in doc:\n",
    "            page_text = page.get_text()\n",
    "            if not page_text:\n",
    "                non_text_page_count += 1\n",
    "                if non_text_page_count >= max_non_text_pages:\n",
    "                    raise ValueError(\"PDF contains too many non-text pages. Please use your own OCR tool first.\")\n",
    "                continue\n",
    "            else:\n",
    "                non_text_page_count = 0  # Reset counter if a text page is found\n",
    "            text += page_text\n",
    "\n",
    "        return text\n",
    "\n",
    "def extract_toc_from_pdf(pdf_path, max_non_text_pages=10):\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        toc = doc.get_toc(simple=False)\n",
    "        non_text_page_count = 0\n",
    "\n",
    "        # Check for non-text pages in the document\n",
    "        for page in doc:\n",
    "            if not page.get_text():\n",
    "                non_text_page_count += 1\n",
    "                if non_text_page_count >= max_non_text_pages:\n",
    "                    raise ValueError(\"PDF contains too many non-text pages. Please use your own OCR tool first.\")\n",
    "                continue\n",
    "            else:\n",
    "                non_text_page_count = 0  # Reset counter if a text page is found\n",
    "                break  # Break the loop after finding the first text page\n",
    "\n",
    "        return toc\n",
    "\n",
    "\n",
    "def extract_notes_from_pdf(pdf_path):\n",
    "    notes = []\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for page in doc:\n",
    "            annotations = page.annots()\n",
    "            if annotations:\n",
    "                for annot in annotations:\n",
    "                    annot_text = annot.info['content']\n",
    "                    if annot_text:\n",
    "                        notes.append(annot_text)\n",
    "    return notes\n",
    "\n",
    "\n",
    "def extract_pdf_metadata(pdf_path):\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        metadata = doc.metadata\n",
    "    return metadata\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    pass\n",
    "\n",
    "\n",
    "def extract_pdf_metadata(file_path):\n",
    "    \"\"\"\n",
    "    Extracts metadata from a PDF file.\n",
    "    Args:\n",
    "    file_path (str): Path to the PDF file.\n",
    "    Returns:\n",
    "    dict: Metadata of the PDF.\n",
    "    \"\"\"\n",
    "    metadata = {}\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        metadata = pdf.metadata\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DocumentManagement.Elastic import IndexSettingsGenerator,search_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch, helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(\n",
    "    ['https://localhost:9200'],\n",
    "    http_auth=('elastic', 'dR8dVIqQ5=i3pPSH00zC'),  # Replace with your credentials\n",
    "    verify_certs=True,\n",
    "    ca_certs='/root/http_ca.crt'  # Path to your CA certificate\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tika import parser\n",
    "\n",
    "parsed = parser.from_file(pdf_paper)\n",
    "print(parsed[\"metadata\"])  # To print the metadata of the document.\n",
    "print(parsed[\"content\"])   # To print the text content of the document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tika import parser\n",
    "import re\n",
    "\n",
    "def extract_text_tika(file_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file using Apache Tika, page by page, and checks for consecutive pages without text.\n",
    "    Args:\n",
    "        file_path (str): Path to the PDF file.\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are page numbers and values are extracted text from each page.\n",
    "    Raises:\n",
    "        ValueError: If 10 consecutive pages are found without text.\n",
    "    \"\"\"\n",
    "    # Parse the PDF file\n",
    "    parsed = parser.from_file(file_path)\n",
    "\n",
    "    # Extract the content\n",
    "    content = parsed['content']\n",
    "    if not content:\n",
    "        raise ValueError(\"No content found in the PDF file.\")\n",
    "\n",
    "    # Split the content into pages\n",
    "    pages = content.split('\\f')  # Splitting by form feed character which typically represents page breaks\n",
    "    text = {}\n",
    "    consecutive_pages_without_text = 0\n",
    "\n",
    "    for i, page_content in enumerate(pages, 1):\n",
    "        # Clean up whitespace\n",
    "        page_text = ' '.join(page_content.strip().split())\n",
    "\n",
    "        if not page_text:\n",
    "            consecutive_pages_without_text += 1\n",
    "            if consecutive_pages_without_text == 10:\n",
    "                raise ValueError(\"PDF seems to contain only images. Please use OCR first.\")\n",
    "        else:\n",
    "            consecutive_pages_without_text = 0\n",
    "\n",
    "        text[i] = page_text\n",
    "\n",
    "    return text\n",
    "\n",
    "# Usage\n",
    "# file_path = 'path_to_your_pdf_file.pdf'\n",
    "# pdf_text = extract_text_tika(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tika import parser\n",
    "import re\n",
    "\n",
    "def extract_text_tika(file_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file using Apache Tika, page by page, and checks for consecutive pages without text.\n",
    "    Args:\n",
    "        file_path (str): Path to the PDF file.\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are page numbers and values are extracted text from each page.\n",
    "    Raises:\n",
    "        ValueError: If 10 consecutive pages are found without text.\n",
    "    \"\"\"\n",
    "    # Parse the PDF file\n",
    "    parsed = parser.from_file(file_path)\n",
    "\n",
    "    # Extract the content\n",
    "    content = parsed['content']\n",
    "    if not content:\n",
    "        raise ValueError(\"No content found in the PDF file.\")\n",
    "\n",
    "    # Split the content into pages\n",
    "    pages = content.split('\\f')  # Splitting by form feed character which typically represents page breaks\n",
    "    text = {}\n",
    "    consecutive_pages_without_text = 0\n",
    "\n",
    "    for i, page_content in enumerate(pages, 1):\n",
    "        # Clean up whitespace\n",
    "        page_text = ' '.join(page_content.strip().split())\n",
    "\n",
    "        if not page_text:\n",
    "            consecutive_pages_without_text += 1\n",
    "            if consecutive_pages_without_text == 10:\n",
    "                raise ValueError(\"PDF seems to contain only images. Please use OCR first.\")\n",
    "        else:\n",
    "            consecutive_pages_without_text = 0\n",
    "\n",
    "        text[i] = page_text\n",
    "\n",
    "    return text\n",
    "\n",
    "# Usage\n",
    "# file_path = 'path_to_your_pdf_file.pdf'\n",
    "# pdf_text = extract_text_tika(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tika import parser\n",
    "import re\n",
    "\n",
    "def extract_text_tika(file_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file using Apache Tika, page by page, and checks for consecutive pages without text.\n",
    "    Args:\n",
    "        file_path (str): Path to the PDF file.\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are page numbers and values are extracted text from each page.\n",
    "    Raises:\n",
    "        ValueError: If 10 consecutive pages are found without text.\n",
    "    \"\"\"\n",
    "    # Parse the PDF file\n",
    "    parsed = parser.from_file(file_path)\n",
    "    content = parsed['content']\n",
    "\n",
    "    # Split the content by pages - Tika's plain text output may not provide clear page delimiters\n",
    "    # Adjust the splitting logic based on the actual output format\n",
    "    pages = content.split('some_page_delimiter')  # Replace 'some_page_delimiter' with the actual delimiter used by Tika\n",
    "\n",
    "    text = {}\n",
    "    consecutive_pages_without_text = 0\n",
    "\n",
    "    for i, page_content in enumerate(pages, start=1):\n",
    "        page_text = page_content.strip()\n",
    "\n",
    "        if not page_text:\n",
    "            consecutive_pages_without_text += 1\n",
    "            if consecutive_pages_without_text == 10:\n",
    "                raise ValueError(\"10 consecutive pages without text found. PDF may contain only images. Please use OCR.\")\n",
    "        else:\n",
    "            consecutive_pages_without_text = 0\n",
    "\n",
    "        text[i] = page_text\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_text = extract_text_tika(pdf_paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from io import BytesIO\n",
    "\n",
    "def add_invisible_markers(pdf_path, marker):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    writer = PdfWriter()\n",
    "\n",
    "    for i in range(len(reader.pages)):\n",
    "        page = reader.pages[i]\n",
    "        packet = BytesIO()\n",
    "        can = canvas.Canvas(packet, pagesize=letter)\n",
    "        can.setFontSize(1)  # Setting font size to 1 or 0 to make it nearly invisible\n",
    "        can.drawString(0, 0, marker)  # Position at bottom-left; can adjust as needed\n",
    "        can.save()\n",
    "\n",
    "        packet.seek(0)\n",
    "        new_pdf = PdfReader(packet)\n",
    "        page.merge_page(new_pdf.pages[0])\n",
    "        writer.add_page(page)\n",
    "\n",
    "    with open(\"/root/gpt_projects/ABoringKnowledgeManagementSystem/DocumentProcessing/tests/modified_pdf.pdf\", \"wb\") as f:\n",
    "        writer.write(f)\n",
    "\n",
    "add_invisible_markers(pdf_paper ,\"__PageBreak__12345__\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tika import parser\n",
    "\n",
    "def extract_pages_with_tika(file_path, marker):\n",
    "    parsed = parser.from_file(file_path)\n",
    "    content = parsed['content']\n",
    "    pages = content.split(marker)  # Splitting text using the marker\n",
    "\n",
    "    text_by_page = {i+1: page.strip() for i, page in enumerate(pages)}\n",
    "    return text_by_page\n",
    "\n",
    "text_by_page = extract_pages_with_tika(\"/root/gpt_projects/ABoringKnowledgeManagementSystem/DocumentProcessing/tests/modified_pdf.pdf\", \"__PageBreak__12345__\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_by_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = pdf_processor.extract_text(pdf_paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_by_page[1], full_text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DocumentIndexing.MongoDB import documentstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = documentstore.get_document_from_collection('research_paper', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "PAGE = \"PDF VIEWER\"\n",
    "pickle.dump( PAGE, open(CHAT_BOT_STATUT_CACHE_PATH, \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (CHAT_BOT_STATUT_CACHE_PATH, 'rb') as fp:\n",
    "    itemlist = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_BOT_STATUT_CACHE_PATH = \"/root/gpt_projects/ABoringKnowledgeManagementSystem/WebApp/WebUI/static/chat_bot_statut_cache/chat_bot_statut_cache.pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_VIEWER_CACHE_PATH = \"/root/gpt_projects/ABoringKnowledgeManagementSystem/WebApp/WebUI/static/pdf_viewer_cache/pdf_viewer_cache.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_FILE = ('d42a0cd9-fdfd-4639-a38c-a85b08cd37a7','research_paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( PDF_FILE, open(PDF_VIEWER_CACHE_PATH, \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (PDF_VIEWER_CACHE_PATH, 'rb') as fp:\n",
    "    itemlist = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DocumentIndexing.Elastic import IndexSettingsGenerator\n",
    "from DocumentIndexing.Elastic import search_engine\n",
    "from elasticsearch.helpers import scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import json\n",
    "\n",
    "def save_research_papers_to_json():\n",
    "    # Connect to MongoDB (Update 'your_connection_string' with your actual connection string)\n",
    "    client = MongoClient(MONGODB_HOST)\n",
    "    replace_string = '/root/gpt_projects/ABoringKnowledgeManagementSystem/DocumentBank'\n",
    "    # Select the database and collection\n",
    "    db = client[MONGODB_DB]# Replace 'your_database_name' with your actual database name\n",
    "    collection = db['research_paper']\n",
    "    \n",
    "    # Query the collection for the needed fields\n",
    "    papers = collection.find({}, {'document_title': 1, 'file_path': 1, 'upload_date': 1, '_id': 0}).sort('upload_date', -1)\n",
    "    \n",
    "    # Format the data into the specified dictionary format\n",
    "    research_papers_dict = {\"research_paper\": [(paper['document_title'], paper['file_path'].replace(replace_string,'/media')) for paper in papers]}\n",
    "    \n",
    "    print(research_papers_dict)\n",
    "    with open(DOCUMENT_FILE_LIST_CACHE_PATH, 'w') as f:\n",
    "        json.dump(research_papers_dict, f)\n",
    "    client.close()\n",
    "# Remember to replace 'your_connection_string' and 'your_database_name'\n",
    "# with the actual connection string to your MongoDB instance and database name.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_research_papers_to_json()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DocumentIndexing.Embedding.embedding import TextEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DocumentIndexing.Elastic.search_engine import SearchEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_engine = SearchEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_engine.search_for_terms('research_paper_chunk_level','bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Chat import rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = rag.two_step_retrival_page(\"what is BERT?\",\"2610a7e7-359f-4e79-85d8-d3e3dff51dab\", \"research_paper\",return_top_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rag.two_step_prompt_page(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = rag.one_document_prompt(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexsetting = IndexSettingsGenerator.GeneralIndexSettings(EMBEDDING_DINENSION)\n",
    "chunk_mapping = indexsetting.chunk_level_mapping()\n",
    "document_mapping = indexsetting.document_level_mapping()\n",
    "page_mapping = indexsetting.page_level_mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch(hosts=ES_HOST, http_auth=ES_HTTP_AUTH, verify_certs=True, ca_certs=ES_CA_CERTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_search_engine = search_engine.SearchEngine()    \n",
    "for mapping,level in zip([chunk_mapping, page_mapping,document_mapping],DOCUMENT_LEVEL):\n",
    "    print(mapping)\n",
    "    response = es.indices.delete(index=f'research_paper_'+level, ignore=[400, 404])\n",
    "    new_search_engine.create_index(index_name=f'research_paper_'+level, index_settings=mapping)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eleasticengine = search_engine.SearchEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index_name in DOCUMENT_TYPE:\n",
    "    eleasticengine.create_index(index_name,mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"research_paper_chunk_level\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import scan\n",
    "\n",
    "# Connect to Elasticsearch\n",
    "es = Elasticsearch(hosts=ES_HOST, http_auth=ES_HTTP_AUTH, verify_certs=True, ca_certs=ES_CA_CERTS_PATH)\n",
    "# Index name\n",
    "index_name = \"research_paper\"\n",
    "\n",
    "# Initialize the scan\n",
    "results = scan(es, index=index_name, query={\"query\": {\"match_all\": {}}})\n",
    "\n",
    "# Iterate over the results\n",
    "for doc in results:\n",
    "    print(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import DOCUMENT_TYPE, ES_HOST, ES_HTTP_AUTH, ES_CA_CERTS_PATH, EMBEDDING_DINENSION,DOCUMENT_LEVEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch(hosts=ES_HOST, http_auth=ES_HTTP_AUTH, verify_certs=True, ca_certs=ES_CA_CERTS_PATH)\n",
    "\n",
    "\n",
    "existing_indices = DOCUMENT_TYPE\n",
    "# Delete each index\n",
    "for index in existing_indices:\n",
    "    response = es.indices.delete(index=index)\n",
    "    print(f\"Deleted index: {index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['hits']['hits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = embedding_local.embeddings_multilingual(\"ﬁnetuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = eleasticengine.vector_search(\"research_paper\",vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2['hits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdftitle\n",
    "print(pdftitle.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdftitle import get_title_from_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = '/root/gpt_projects/ABoringKnowledgeManagementSystem/DocumentBank/research_paper/416f1ec4-bcbd-4204-9069-b6ea86c1b7a9.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    title = get_title_from_file(fp)\n",
    "    print(f\"The extracted title is: {title}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'DocumentBank/research_paper/2610a7e7-359f-4e79-85d8-d3e3dff51dab.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp= \"/root/downloads/MY_CV_Final.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DocumentManagement.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Document(file_path = 'DocumentBank/research_paper/2610a7e7-359f-4e79-85d8-d3e3dff51dab.pdf',document_type='research_paper',document_id='123')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DocumentProcessing.pdf_processing import pdf_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_processor.extract_pdf_metadata(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_processor.structured_metadata_for_paper(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from llama_hub.file.unstructured import UnstructuredReader\n",
    "\n",
    "loader = UnstructuredReader()\n",
    "documents = loader.load_data(file=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index.ingestion import IngestionPipeline\n",
    "from llama_index.node_parser import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = IngestionPipeline(transformations=[TokenTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=20,\n",
    "    separator=\" \",\n",
    ")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.text_splitter import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = pipeline.run(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.text_splitter import TokenTextSplitter\n",
    "\n",
    "splitter = TokenTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=20,\n",
    "    separator=\" \",\n",
    ")\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex(nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "es = Elasticsearch(hosts=ES_HOST, http_auth=ES_HTTP_AUTH, verify_certs=True, ca_certs=ES_CA_CERTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.elasticsearch import ElasticsearchEmbedding\n",
    "from llama_index.vector_stores import ElasticsearchStore\n",
    "from llama_index import ServiceContext, StorageContext, VectorStoreIndex\n",
    "\n",
    "\n",
    "\n",
    "from llama_index.vector_stores import ElasticsearchStore\n",
    "\n",
    "vector_store = ElasticsearchStore(\n",
    "    index_name= index_name,\n",
    "    es_client=es,     \n",
    "    vwctor_field=\"text_piece_vector\",  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'We also undertake systematic study of “data contamination” – growing problem when training high capacity models\\non datasets such as Common Crawl which can potentially include content from test datasets simply because such\\ncontent often exists on the web In this paper we develop systematic tools to measure data contamination and quantify\\nits distorting effects Although we ﬁnd that data contamination has minimal effect on GPT’ performance on most\\ndatasets we do identify few datasets where it could be inﬂating results and we either do not report results on these\\ndatasets or we note them with an asterisk depending on the severity\\nIn addition to all the above we also train series of smaller models ranging from 125 million parameters to 13 billion\\nparameters in order to compare their performance to GPT in the zero one and fewshot settings Broadly for most\\ntasks we ﬁnd relatively smooth scaling with model capacity in all three settings one notable pattern is that the gap\\nbetween zero one and fewshot performance often grows with model capacity perhaps suggesting that larger models\\nare more proﬁcient metalearners\\nFinally given the broad spectrum of capabilities displayed by GPT we discuss concerns about bias fairness and\\nbroader societal impacts and attempt preliminary analysis of GPT’ characteristics in this regard\\nThe remainder of this paper is organized as follows In Section we describe our approach and methods for training\\nGPT and evaluating it Section presents results on the full range of tasks in the zero one and fewshot settings\\nSection addresses questions of data contamination traintest overlap Section discusses limitations of GPT\\nSection discusses broader impacts Section reviews related work and Section concludes\\n Approach\\nOur basic pretraining approach including model data and training is similar to the process described in RWC19\\nwith relatively straightforward scaling up of the model size dataset size and diversity and length of training Our use\\nof incontext learning is also similar to RWC19 but in this work we systematically explore different settings for\\nlearning within the context Therefore we start this section by explicitly deﬁning and contrasting the different settings\\nthat we will be evaluating GPT on or could in principle evaluate GPT on These settings can be seen as lying on \\nspectrum of how much taskspeciﬁc data they tend to rely on Speciﬁcally we can identify at least four points on this\\nspectrum see Figure for an illustration\\n• FineTuning FT has been the most common approach in recent years and involves updating the weights of\\n pretrained model by training on supervised dataset speciﬁc to the desired task Typically thousands to\\nhundreds of thousands of labeled examples are used The main advantage of ﬁnetuning is strong performance\\non many benchmarks The main disadvantages are the need for new large dataset for every task the potential\\nfor poor generalization outofdistribution MPL19 and the potential to exploit spurious features of the\\ntraining data GSL18 NK19 potentially resulting in an unfair comparison with human performance In\\nthis work we do not ﬁnetune GPT because our focus is on taskagnostic performance but GPT can be\\nﬁnetuned in principle and this is promising direction for future work\\n• FewShot FS is the term we will use in this work to refer to the setting where the model is given few\\ndemonstrations of the task at inference time as conditioning RWC19 but no weight updates are allowed\\nAs shown in Figure for typical dataset an example has context and desired completion for example\\nan English sentence and the French translation and fewshot works by giving examples of context and\\ncompletion and then one ﬁnal example of context with the model expected to provide the completion We\\ntypically set in the range of 10 to 100 as this is how many examples can ﬁt in the model’ context window\\nnctx 2048 The main advantages of fewshot are major reduction in the need for taskspeciﬁc data and\\nreduced potential to learn an overly narrow distribution from large but narrow ﬁnetuning dataset The main\\ndisadvantage is that results from this method have so far been much worse than stateoftheart ﬁnetuned\\nmodels Also small amount of task speciﬁc data is still required As indicated by the name fewshot\\nlearning as described here for language models is related to fewshot learning as used in other contexts in\\nML HYC01 VBL16 – both involve learning based on broad distribution of tasks in this case implicit in\\nthe pretraining data and then rapidly adapting to new task\\n• OneShot 1S is the same as fewshot except that only one demonstration is allowed in addition to natural\\nlanguage description of the task as shown in Figure The reason to distinguish oneshot from fewshot and\\nzeroshot below is that it most closely matches the way in which some tasks are communicated to humans\\nFor example when asking humans to generate dataset on human worker service for example Mechanical\\nTurk it is common to give one demonstration of the task By contrast it is sometimes difﬁcult to communicate\\nthe content or format of task if no examples are given'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DocumentIndexing.Embedding import embedding_toolkits\n",
    "from DocumentIndexing.Embedding import text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_toolkits.w2v_token_len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_toolkits.token_length(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter.split_text_with_langchain(text,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('distiluse-base-multilingual-cased-v2')\n",
    "token = model.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(token['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/distiluse-base-multilingual-cased-v2')\n",
    "encoded_input = tokenizer.encode_plus(text, add_special_tokens=True, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = encoded_input['input_ids'][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([token for token in input_ids if token not in tokenizer.all_special_ids])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0, model_name=\"sentence-transformers/distiluse-base-multilingual-cased-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter.maximum_tokens_per_chunk = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter.count_tokens(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/distiluse-base-multilingual-cased-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split= splitter.split_text(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter.count_tokens(text=split[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load your SentenceTransformer model\n",
    "model_name = \"sentence-transformers/distiluse-base-multilingual-cased-v2\"  # Replace with your model name\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Access the underlying Hugging Face model\n",
    "hf_model = model._first_module().auto_model\n",
    "\n",
    "# Check the maximum input length\n",
    "max_length = hf_model.config.max_position_embeddings\n",
    "print(\"Maximum input token limit:\", max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacypdfreader.spacypdfreader import pdf_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = pdf_reader('/root/gpt_projects/ABoringKnowledgeManagementSystem/DocumentBank/research_paper/06063c65-7cc9-4065-8c26-03d8d7aa6379.pdf', nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc._.page(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = []\n",
    "for i in range (1,2):\n",
    "    for token in doc._.page(i):\n",
    "        if token.text.endswith('\\n'):\n",
    "        # Remove the line break and do not add a space\n",
    "            new_text += token.text.rstrip('\\n')\n",
    "        else:\n",
    "            # Add token text with a following space\n",
    "            new_text += token.text_with_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for token in list(doc._.page(1).sents):\n",
    "    if token.text.endswith('\\n'):\n",
    "        # Remove the line break and do not add a space\n",
    "        new_text += token.text.rstrip('\\n')\n",
    "    else:\n",
    "        # Add token text with a following space\n",
    "        new_text += token.text_with_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = list(doc._.page(1).sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents[0].text_with_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer\n",
    "for page_layout in extract_pages('/root/gpt_projects/ABoringKnowledgeManagementSystem/DocumentBank/research_paper/06063c65-7cc9-4065-8c26-03d8d7aa6379.pdf'):\n",
    "    for element in page_layout:\n",
    "        if isinstance(element, LTTextContainer):\n",
    "            print(element.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DocumentProcessing.pdf_processing import pdf_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor  = pdf_processor.PDFProcessor('/root/gpt_projects/ABoringKnowledgeManagementSystem/DocumentBank/research_paper/06063c65-7cc9-4065-8c26-03d8d7aa6379.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = processor.extract_text_from_pdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DocumentIndexing.Embedding import text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spliter = text_splitter.TextSplitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spliter.split_text_with_langchain_recursive(pages[1],128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spliter.split_text_with_langchain_sentence_transformers(pages[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DocumentIndexing.Embedding.embedding_toolkits import token_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_sentences(sentences, max_tokens=128):\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_token_count = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Tokenize the sentence and count the tokens\n",
    "        token_count = token_length(sentence.text)\n",
    "\n",
    "        # If adding this sentence exceeds the max token count, start a new chunk\n",
    "        if current_token_count + token_count > max_tokens:\n",
    "            if current_chunk:  # Ensure current chunk is not empty\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence.text]\n",
    "            current_token_count = token_count\n",
    "        else:\n",
    "            # Add the sentence to the current chunk\n",
    "            current_chunk.append(sentence.text)\n",
    "            current_token_count += token_count\n",
    "\n",
    "    # Add the last chunk if it's not empty\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_sentences(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[chunks.replace('\\n', '') for chunks in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_sentences_with_sentence_overlap(sentences, max_tokens=128, overlap=0):\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_token_count = 0\n",
    "    overlap_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        token_count = token_length(sentence)\n",
    "\n",
    "        if current_token_count + token_count > max_tokens:\n",
    "            if current_chunk:  # Ensure current chunk is not empty\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "            # Start new chunk with overlap from previous chunk\n",
    "            current_chunk = overlap_sentences + [sentence]\n",
    "            current_token_count = len(' '.join(current_chunk).split())\n",
    "            # Update overlap sentences\n",
    "            overlap_sentences = current_chunk[-overlap:] if len(current_chunk) > overlap else current_chunk[:]\n",
    "        else:\n",
    "            # Add the sentence to the current chunk\n",
    "            current_chunk.append(sentence)\n",
    "            current_token_count += token_count\n",
    "            # Update overlap sentences\n",
    "            if len(current_chunk) > overlap:\n",
    "                overlap_sentences = current_chunk[-overlap:]\n",
    "\n",
    "    # Add the last chunk if it's not empty\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_o = chunk_sentences(sents, overlap=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[chunks.replace('\\n', '') for chunks in chunks_o]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_sentences_with_sentence_overlap_generator(sentences, max_tokens=128, overlap=0):\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_token_count = 0\n",
    "    overlap_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        token_count = token_length(sentence.text)\n",
    "\n",
    "        if current_token_count + token_count > max_tokens:\n",
    "            if current_chunk:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "            # Start new chunk with overlap from previous chunk\n",
    "            current_chunk = overlap_sentences + [sentence.text]\n",
    "            current_token_count = len(' '.join(current_chunk).split())\n",
    "            # Prepare overlap sentences for next chunk\n",
    "            overlap_sentences = current_chunk[-overlap:] if len(current_chunk) > overlap else current_chunk[:]\n",
    "        else:\n",
    "            current_chunk.append(sentence.text)\n",
    "            current_token_count += token_count\n",
    "            if len(current_chunk) > overlap:\n",
    "                overlap_sentences = current_chunk[-overlap:]\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_g = chunk_sentences_with_sentence_overlap_generator(doc._.page(1).sents, overlap=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[chunks.replace('\\n', '') for chunks in chunks_g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DocumentIndexing.Embedding.text_splitter import TextSplitter_Spacy\n",
    "from DocumentProcessing.pdf_processing.pdf_processor import SpacyPdfProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_processor = SpacyPdfProcessor('/root/gpt_projects/ABoringKnowledgeManagementSystem/WebApp/d42a0cd9-fdfd-4639-a38c-a85b08cd37a7.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = spacy_processor.extract_text_from_pdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pages[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spliter = TextSplitter_Spacy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = spliter.chunk_sentences_with_sentence_overlap(pages[1], 128,overlap=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_generator = spacy_processor.generate_senteces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(spacy_processor.first,spacy_processor.last+1):\n",
    "    print(spliter.chunk_sentences_with_sentence_overlap(next(page_generator)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_generator = spacy_processor.generate_senteces_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_nb, page = next(page_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = spliter.chunk_sentences_with_sentence_overlap_generator(page, 128,overlap=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_nb,split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DocumentManagement.documents import PDFDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = PDFDocument(file_path = '/root/gpt_projects/ABoringKnowledgeManagementSystem/DocumentBank/research_paper/06063c65-7cc9-4065-8c26-03d8d7aa6379.pdf',document_type='research_paper',document_id='123')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = arxiv.Client()\n",
    "\n",
    "search_by_id = arxiv.Search(id_list=[\"1706.03762\"])\n",
    "# Reuse client to fetch the paper, then print its title.\n",
    "first_result = list(client.results(search_by_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_result[0].categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def is_valid_arxiv_id(arxiv_id):\n",
    "    # Regular expression pattern for arXiv IDs\n",
    "    # Format: arXiv:YYMM.number{vV}\n",
    "    # YYMM - two-digit year and month\n",
    "    # number - zero-padded sequence number, 4-digits from 0704 to 1412, 5-digits from 1501 onwards\n",
    "    # vV - optional version number\n",
    "    pattern = r'(\\d{2})(\\d{2})\\.(\\d{4,5})(v\\d+)?$'\n",
    "    \n",
    "    match = re.match(pattern, arxiv_id)\n",
    "    if not match:\n",
    "        return False\n",
    "\n",
    "    # Extract year and month to handle the change in number length\n",
    "    year, month, _, _ = match.groups()\n",
    "    year, month = int(year), int(month)\n",
    "\n",
    "    # Check the number of digits in 'number' based on year and month\n",
    "    if year == 14 and month <= 12 or year < 14:\n",
    "        return len(match.group(3)) == 4  # Should be 4 digits\n",
    "    else:\n",
    "        return len(match.group(3)) == 5  # Should be 5 digits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_valid_arxiv_id(\"1501.00001v4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# list of sentences\n",
    "sentences = ['sentence_0', 'sentence_1', ...]\n",
    "\n",
    "# init embedding model\n",
    "## sentence-trnasformers支持有更新，请注意先删除本地模型缓存：\"`SENTENCE_TRANSFORMERS_HOME`/maidalun1020_bce-embedding-base_v1\"或“～/.cache/torch/sentence_transformers/maidalun1020_bce-embedding-base_v1”\n",
    "model = SentenceTransformer(\"maidalun1020/bce-embedding-base_v1\",use_auth_token=\"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\")\n",
    "\n",
    "# extract embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode(split, normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# init reranker model\n",
    "r_model = CrossEncoder('/root/.cache/huggingface/hub/models--maidalun1020--bce-reranker-base_v1/snapshots/eaa31a577a0574e87a08959bd229ca14ce1b5496', max_length=512)\n",
    "\n",
    "\n",
    "# calculate scores of sentence pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = r_model.predict(('What is BLEU score', 'BLEU score is a metric for evaluating a generated sentence to a reference sentence.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sentence_transformers import CrossEncoder\n",
    "import os\n",
    "\n",
    "# Replace 'your_token' with your actual Hugging Face access token\n",
    "\n",
    "\n",
    "# Authenticate with Hugging Face\n",
    "model_name = 'maidalun1020/bce-reranker-base_v1'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token='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')\n",
    "model = AutoModel.from_pretrained(model_name, use_auth_token='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')\n",
    "\n",
    "# Initialize CrossEncoder with the loaded model and tokenizer\n",
    "cross_encoder = CrossEncoder(model, max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import file_utils\n",
    "\n",
    "cache_dir = file_utils.default_cache_path\n",
    "print(cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "model = BGEM3FlagModel('BAAI/bge-m3',  use_fp16=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_1 = model.encode(split, return_dense=True, return_sparse=False, return_colbert_vecs=False,max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# init embedding model\n",
    "## sentence-trnasformers支持有更新，请注意先删除本地模型缓存：\"`SENTENCE_TRANSFORMERS_HOME`/maidalun1020_bce-embedding-base_v1\"或“～/.cache/torch/sentence_transformers/maidalun1020_bce-embedding-base_v1”\n",
    "model = SentenceTransformer('BAAI/bge-m3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.encode(['test1','test2'], normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "EMBEDDING_API_URL = \"http://localhost:5000/embeddings/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"text_list\": ['test sentence 1', 'test sentence 2'],\n",
    "    \"embedding_type\": \"local\"  # or \"oai\" if you want to use the OpenAI API\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(EMBEDDING_API_URL, json=payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = response.json()[\"embeddings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(v).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'research_paper' : [\n",
    "        ('ORCA2','/media/research_paper/6eaf87a4-e5fa-482b-8007-d978c784c8f1.pdf'),\n",
    "        ('BERT','/media/research_paper/b6a53d7f-ab4a-4d4c-bff3-172c2877e67b.pdf'),\n",
    "    ]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/root/gpt_projects/ABoringKnowledgeManagementSystem/WebApp/WebUI/static/document_list_cache/documents.json', 'w') as f:\n",
    "    json.dump(d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "project_options = ['conferece_paper']\n",
    "tag_options = [\n",
    "    ('Linguistics', 'Linguistics'),\n",
    "    (\"Linguistics>Phonetics\", \"Linguistics>Phonetics\"),\n",
    "    (\"Linguistics>Phonology\", \"Linguistics>Pphonology\"),\n",
    "    (\"Linguistics>Morphology\", \"Linguistics>Morphology\"),\n",
    "    (\"Linguistics>Syntax\", \"Linguistics>Syntax\"),\n",
    "    (\"Linguistics>Semantics\", \"Linguistics>Semantics\"),\n",
    "    (\"Linguistics>Pragmatics\", \"Linguistics>Pragmatics\"),\n",
    "    (\"Linguistics>NLP\", \"Linguistics>NLP\"),\n",
    "    (\"Machine Learning\", \"Machine Learning\"),\n",
    "    \n",
    "    \n",
    "]\n",
    "tag_options = [tag[0] for tag in tag_options]\n",
    "\n",
    "tags_and_projects_cache = {'project_options': project_options, 'tag_options': tag_options}\n",
    "pickle.dump(tags_and_projects_cache, open('/root/gpt_projects/ABoringKnowledgeManagementSystem/WebApp/WebUI/static/tags_and_projects_cache/tags_and_projects_cache.pkl', \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "note_cache_path = '/root/gpt_projects/ABoringKnowledgeManagementSystem/Chat/notes.pkl'\n",
    "example_notes = [{'id': 1, 'type':'document_note','attached_document_id':'12344556','title' : 'Title 1', 'content' : 'Content 1'}, {'id':2,'title' : 'Title 2', 'content' : 'Content 2'}]\n",
    "pickle.dump( example_notes, open(note_cache_path, \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import MONGODB_HOST, MONGODB_DB\n",
    "from DocumentIndexing.MongoDB.documentstore import get_document_from_mongodb_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = get_document_from_mongodb_id('research_paper', '6eaf87a4-e5fa-482b-8007-d978c784c8f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = next(a)['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =''\n",
    "for i in a.values():\n",
    "    text += ''.join(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define prompt\n",
    "prompt_template = \"\"\"Write a concise summary of the following:\n",
    "\"{text}\"\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4-1106-preview\",openai_api_key=os.environ['OPENAI_API_KEY '])\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs =  [Document(page_content=text, metadata={\"source\": \"local\"})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stuff_chain.run(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.run(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "\n",
    "# Example dataset: 2D points\n",
    "X = np.array([[1, 2], [1, 4], [1, 0],\n",
    "              [10, 2], [10, 4], [10, 0]])\n",
    "\n",
    "# Normalize the dataset to unit length\n",
    "X_normalized = normalize(X, norm='l2')\n",
    "\n",
    "# Number of clusters\n",
    "k = 2\n",
    "\n",
    "# Initializing KMeans with desired number of clusters\n",
    "kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "\n",
    "# Fitting the model to the normalized data\n",
    "kmeans.fit(X_normalized)\n",
    "\n",
    "# Cluster labels for each point\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Coordinates of cluster centers\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "labels, cluster_centers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DocumentIndexing.Elastic import search_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_search_engine = search_engine.SearchEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = new_search_engine.serach_by_id('research_paper_chunk_level','ee6977e5-854d-44b8-ae90-d52d64808b5d',1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectos = [i['_source']['text_piece_vector'] for i in search['hits']['hits']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/codebox39/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "v_norm = normalize(vectos, norm='l2')\n",
    "k = 15\n",
    "kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "kmeans.fit(v_norm)\n",
    "labels = kmeans.labels_\n",
    "cluster_centers = kmeans.cluster_centers_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catanzaro. Raven: In-context learning with retrieval aug-mented encoder-decoder language models. arXiv preprintarXiv:2308.07922, 2023. [ILIN, 2023] IVAN ILIN. Advanced rag techniques:https://pub.towardsai.net/an illustrated overview.advanced-rag-techniques-an-illustrated-overview-04d193d8fec6,2023. Advanced rag techniques:https://pub.towardsai.net/an illustrated overview.advanced-rag-techniques-an-illustrated-overview-04d193d8fec6,2023. Few-shot[Izacard et al., 2022] Gautier Izacard, Patrick Lewis, MariaLomeli, Lucas Hosseini, Fabio Petroni, Timo Schick,Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel,learning with re-and Edouard Grave. arXiv preprinttrieval augmented language models. arXiv:2208.03299, 2022. [Jarvis and Allard, 2023]\n",
      "Byemploying a retrieval-enhanced generative model that uses itsown outputs to improve itself, the text becomes more alignedwith the data distribution during the reasoning process. Con-sequently, the model’s own outputs are utilized instead of thetraining data [Wang et al., 2022a]. Fusion. RAG-Fusion [Raudaschl, 2023]enhances tradi-tional search systems by addressing their limitations througha multi-query approach that expands user queries into mul-tiple, diverse perspectives using an LLM. CREA-ICL [Li et al., 2023b] employs a synchronous retrieval ofcross-lingual knowledge to enhance context, while RE-CITE [Sun et al., 2022] generates context by sampling para-graphs directly from LLMs. Further refinement of the RAG process during infer-ence is seen in approaches that cater to tasks necessi-ITRG [Feng et al., 2023] it-tating multi-step reasoning.eratively retrieves information to identify the correct rea-ITER-soning paths, thereby improving task adaptability.\n"
     ]
    }
   ],
   "source": [
    "r0 = new_search_engine.vector_search('research_paper_chunk_level',cluster_centers[0],document_id = 'ee6977e5-854d-44b8-ae90-d52d64808b5d')\n",
    "r1 = new_search_engine.vector_search('research_paper_chunk_level',cluster_centers[1],document_id = 'ee6977e5-854d-44b8-ae90-d52d64808b5d')\n",
    "\n",
    "text0 = r0['hits']['hits'][0]['_source']['text_piece']\n",
    "text0_1 = r0['hits']['hits'][1]['_source']['text_piece']\n",
    "text1 = r1['hits']['hits'][0]['_source']['text_piece']\n",
    "text1_1 = r1['hits']['hits'][1]['_source']['text_piece']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Catanzaro. Raven: In-context learning with retrieval aug-mented encoder-decoder language models. arXiv preprintarXiv:2308.07922, 2023. [ILIN, 2023] IVAN ILIN. Advanced rag techniques:https://pub.towardsai.net/an illustrated overview.advanced-rag-techniques-an-illustrated-overview-04d193d8fec6,2023.',\n",
       " 'Advanced rag techniques:https://pub.towardsai.net/an illustrated overview.advanced-rag-techniques-an-illustrated-overview-04d193d8fec6,2023. Few-shot[Izacard et al., 2022] Gautier Izacard, Patrick Lewis, MariaLomeli, Lucas Hosseini, Fabio Petroni, Timo Schick,Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel,learning with re-and Edouard Grave. arXiv preprinttrieval augmented language models. arXiv:2208.03299, 2022. [Jarvis and Allard, 2023]')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text0, text0_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Chat import pre_build_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/codebox39/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Selfmem [Cheng et al., 2023b] iteratively creates anunbounded memory pool with a retrieval-enhanced genera-tor, using a memory selector to choose outputs that serve asdual problems to the original question, thus self-enhancingthe generative model. These methodologies underscore the breadth of innovativedata source utilization in RAG, striving to improve model per-formance and task effectiveness. 6.3 Augmentation Process In the domain of RAG, the standard practice often involvesa singular retrieval step followed by generation, which cantermed the “lostlead to inefficiencies.\\n[Khattab et al., 2022] introduces the Demonstrate-Search-Predict framework, treating the context learning sys-tem as an explicit program rather than a final task prompt,leading to more effective handling of knowledge-intensivetasks. The ITER-RETGEN [Shao et al., 2023] approach uti-lizes generated content to guide retrieval,iteratively im-plementing “retrieval-enhanced generation” and “generation-enhanced retrieval” within a Retrieve-Read-Retrieve-Readflow. This method demonstrates an innovative way of usingone module’s output to improve the functionality of another. Optimizing the RAG Pipeline The optimization of the retrieval process aims to enhance theefficiency and quality of information in RAG systems.',\n",
       " 'In thisframework, a query initiated by a user prompts the retrieval ofpertinent information via search algorithms. This informationis then woven into the LLM’s prompts, providing additionalcontext for the generation process. RAG’s key advantage liesin its obviation of the need for retraining of LLMs for task-specific applications. Developers can instead append an ex-ternal knowledge repository, enriching the input and therebyrefining the model’s output precision.\\n[Lewis et al., 2020] inmid-2020, stands as a paradigm within the realm of LLMs,enhancing generative tasks. Specifically, RAG involves aninitial retrieval step where the LLMs query an external datasource to obtain relevant information before proceeding to an-swer questions or generate text. This process not only informsthe subsequent generation phase but also ensures that the re-sponses are grounded in retrieved evidence, thereby signif-icantly enhancing the accuracy and relevance of the output.',\n",
       " '[Brown et al., 2020] Tom Brown, Benjamin Mann, Nick Ry-der, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,Amanda Askell, et al. Language models are few-shotlearners. Advances in neural information processing sys-tems, 33:1877–1901, 2020. [Cai et al., 2021] Deng Cai, Yan Wang, Huayang Li, WaiNeural machine translationarXiv preprintLam, and Lemao Liu. with monolingual translation memory. arXiv:2105.11269, 2021.\\nMengzhou Xia, Guoping Huang, LemaoLiu, and Shuming Shi. Graph based translation mem- In Proceedings ofory for neural machine translation. the AAAI conference on artificial intelligence, volume 33,pages 7297–7304, 2019. [Xiao et al., 2023] Guangxuan Xiao, Yuandong Tian, BeidiChen, Song Han, and Mike Lewis. Efficient stream-ing language models with attention sinks. arXiv preprintarXiv:2309.17453, 2023. [Xu et al., 2023a]',\n",
       " '5 GenerationA crucial component of RAG is its generator, which is re-sponsible for converting retrieved information into coherentand fluent text. Unlike traditional language models, RAG’sgenerator sets itself apart by improving accuracy and rele-vance via the incorporation of retrieved data. In RAG, thegenerator’s input encompasses not only typical contextual in-formation but also relevant text segments obtained throughthe retriever. This comprehensive input enables the generatorto gain a deep understanding of the question’s context, result-ing in more informative and contextually relevant responses.\\n[Li et al., 2023d] into documents to rectify align-ment issues and differences. RetrievalDuring the retrieval stage, the primary focus is on identifyingthe appropriate context by calculating the similarity betweenthe query and chunks. The embedding model is central tothis process. In the advanced RAG, there is potential for op-timization of the embedding models. Fine-tuning Embedding. Fine-tuning embedding modelssignificantly impact the relevance of retrieved content in RAGsystems.',\n",
       " '[Li et al., 2023d]. This approach improves the re-triever’s ability to recognize structured information throughtwo pre-training strategies: firstly, by utilizing the inher-ent alignment between structured and unstructured data toguide contrastive learning in a structured-aware pre-trainingscheme; and secondly, by employing Masked Entity Predic-tion. The latter uses an entity-centric masking strategy toprompt language models to predict and complete the maskedentities, thus promoting a more profound comprehension ofstructured data.\\nIt enhances the retriever’s sen-sitivity to structured information through two pre-trainingstrategies: first, by leveraging the intrinsic alignment betweenstructured and unstructured data to inform contrastive learn-ing in a structured-aware pre-training scheme; and second, byimplementing Masked Entity Prediction. The latter utilizesan entity-centric masking strategy that encourages languagemodels to predict and fill in the masked entities, thereby fos-tering a deeper understanding of structured data. The issue of aligning queries with structured exter-nal documents, especially when dealing with the dispar-ity between structured and unstructured data, is tackled bySANTA [Li et al., 2023d].',\n",
       " '• We identify and discuss the central technologies integralto the RAG process, specifically focusing on the aspectsof “Retrieval”, “Generator” and “Augmentation”, anddelve into their synergies, elucidating how these com-ponents intricately collaborate to form a cohesive andeffective RAG framework. • We construct a thorough evaluation framework for RAG,outlining the evaluation objectives and metrics. Ourcomparative analysis clarifies the strengths and weak-nesses of RAG compared to fine-tuning from various\\x0c\\nThis ex-ploration covers the objectives of RAG evaluation, the aspectsalong which these models are assessed, and the benchmarksand tools available for such evaluations. The aim is to offer acomprehensive overview of RAG model evaluation, outliningthe methodologies that specifically address the unique aspectsof these advanced generative systems. 7.1 Evaluation TargetsThe assessment of RAG models mainly revolves around twokey components: the retrieval and generation modules. Thisdivision ensures a thorough evaluation of both the quality ofcontext provided and the quality of content produced.',\n",
       " 'Ernie: Enhancedlanguage representation with informative entities. arXivpreprint arXiv:1905.07129, 2019. [Zhang et al., 2023a] Peitian Zhang, Shitao Xiao, ZhengLiu, Zhicheng Dou, and Jian-Yun Nie. Retrieve any-thing to augment large language models. arXiv preprintarXiv:2310.07554, 2023. [Zhang et al., 2023b]\\nZhihong Shao, Yeyun Gong, YelongShen, Minlie Huang, Nan Duan, and Weizhu Chen. En-hancing retrieval-augmented large language models witharXiv preprintiterative retrieval-generation synergy. arXiv:2305.15294, 2023. [Shi et al., 2023] Weijia Shi, Sewon Min, Michihiro Ya-sunaga, Minjoon Seo, Rich James, Mike Lewis, LukeZettlemoyer, and Wen-tau Yih.Replug: Retrieval-augmented black-box language models. arXiv preprintarXiv:2301.12652, 2023.',\n",
       " 'Enhancing data granularity aims to elevate text standard-ization, consistency, factual accuracy, and rich context to im-prove the RAG system’s performance. This includes remov-ing irrelevant information, dispelling ambiguity in entitiesand terms, confirming factual accuracy, maintaining context,and updating outdated documents. Optimizing index structures involves adjusting the size ofchunks to capture relevant context, querying across multipleindex paths, and incorporating information from the graphstructure to capture relevant context by leveraging relation-ships between nodes in a graph data index.\\nOptimizing index structures involves adjusting the size ofchunks to capture relevant context, querying across multipleindex paths, and incorporating information from the graphstructure to capture relevant context by leveraging relation-ships between nodes in a graph data index. Adding metadata information involves integrating refer-enced metadata, such as dates and purposes, into chunks forfiltering purposes, and incorporating metadata like chaptersand subsections of references to improve retrieval efficiency. Alignment optimization addresses alignment issues anddisparities between documents by introducing “hypotheticalquestions” [Li et al., 2023d] into documents to rectify align-ment issues and differences.',\n",
       " 'The LlamaIn-dex [Liu, 2023] introduces a suite of pivotal classes and func-tions designed to enhance the embedding model fine-tuningworkflow, thereby simplifying these intricate processes. Bycurating a corpus infused with domain knowledge and lever-aging the methodologies offered, one can adeptly fine-tune anembedding model to align closely with the specific require-ments of the target domain. Fine-tuning for Downstream Tasks. Fine-tuning embed-ding models for downstream tasks is a critical step in en-hancing model performance.\\nMoreover, task-specific fine-tuning of embedding modelsis essential to ensure that the model comprehends the userquery in terms of content relevance. A model without fine-tuning may not adequately address the requirements of a spe-cific task. Consequently, fine-tuning an embedding model be-comes crucial for downstream applications. There are twoprimary paradigms in embedding fine-tuning methods. Domain Knowledge Fine-tuning.',\n",
       " 'By aligning the retriever’s capabilities with the prefer-ences of the LLMs through feedback signals, both canbe better coordinated [Yu et al., 2023b, Izacard et al., 2022,Yang et al., 2023b, Shi et al., 2023]. Fine-tuning the retrieverfor specific downstream tasks can lead to improved adapt-ability [cite]. The introduction of task-agnostic fine-tuningaims to enhance the retriever’s versatility in multi-task sce-narios [Cheng et al., 2023a].\\nWhile these methods improve semantic representationby incorporating domain knowledge and task-specific fine-tuning, retrievers may not always exhibit optimal compatibil-ity with certain LLMs. To address this, some researchers haveexplored direct supervision of the fine-tuning process usingfeedback from LLMs. This direct supervision seeks to alignthe retriever more closely with the LLM, thereby improvingperformance on downstream tasks. A more comprehensivediscussion on this topic is presented in Section 4.3.',\n",
       " 'This enhancesthe accuracy and credibility of the models, particu-larly for knowledge-intensive tasks, and allows forcontinuous knowledge updates and integration ofdomain-specific information. RAG synergisticallymerges LLMs’ intrinsic knowledge with the vast,dynamic repositories of external databases. Thiscomprehensive review paper offers a detailedexamination of the progression of RAG paradigms,encompassing the Naive RAG, the Advanced RAG,and the Modular RAG. It meticulously scrutinizesthe tripartite foundation of RAG frameworks,which includes the retrieval , the generation andthe augmentation techniques.\\nOur sur-vey illustrates the evolution of RAG technologies and theirimpact on knowledge-intensive tasks. Our analysis delin-eates three developmental paradigms within the RAG frame-work: Naive, Advanced, and Modular RAG, each markinga progressive enhancement over its predecessors. The Ad-vanced RAG paradigm extends beyond the Naive approachby incorporating sophisticated architectural elements, includ-ing query rewriting, chunk reranking, and prompt summariza-tion. These innovations have led to a more nuanced and mod-ular architecture that enhances both the performance and theinterpretability of LLMs.',\n",
       " 'These quality scoresfulness, and answerrelevance.evaluate the efficiency of the RAG model from differ-ent perspectives in the process of information retrievaland generation [Es et al., 2023, Saad-Falcon et al., 2023,Jarvis and Allard, 2023]. The quality scores—context rele-vance, answer faithfulness, and answer relevance—assess theRAG model’s efficiency from various angles throughout theinformation retrieval and generation process\\nThe quality scores—context rele-vance, answer faithfulness, and answer relevance—assess theRAG model’s efficiency from various angles throughout theinformation retrieval and generation process [Es et al., 2023,Saad-Falcon et al., 2023, Jarvis and Allard, 2023].Context Relevance evaluates the precision and specificityof the retrieved context, ensuring relevance and minimizingprocessing costs associated with extraneous content. Answer Faithfulness ensures that the generated answers re-main true to the retrieved context, maintaining consistency\\x0c',\n",
       " '1.1 IntroductionLarge language models (LLMs) such as the GPT se-ries [Brown et al., 2020, OpenAI, 2023] and the LLama se-ries [Touvron et al., 2023], along with other models like[Google, 2023], have achieved remarkable suc-Geminicess in natural language processing, demonstrating supe-∗Corresponding Author.Email:haofen.wang@tongji.edu.cn1Resources are available at https://github.com/Tongji-KGLLM/RAG-Surveyincorrectrior performance on various benchmarks including Super-GLUE [Wang et al., 2019], MMLU [Hendrycks et al., 2020],and BIG-bench [Srivastava et al., 2022].\\nFurthermore, this paper introducesthe metrics and benchmarks for assessing RAGmodels, along with the most up-to-date evaluationIn conclusion, the paper delineatesframework.including theprospective avenues for research,identification of challenges,the expansion ofmulti-modalities, and the progression of the RAGinfrastructure and its ecosystem. 1.1 IntroductionLarge language models (LLMs) such as the GPT se-ries [Brown et al., 2020, OpenAI, 2023] and the LLama se-ries [Touvron et al., 2023], along with other models like[Google, 2023], have achieved remarkable suc-Geminicess in natural language processing, demonstrating supe-∗Corresponding Author.Email:haofen.wang@tongji.edu.cn1Resources are available at https://github.com/Tongji-KGLLM/RAG-Surveyincorrectrior performance on various benchmarks including Super-GLUE',\n",
       " '[Yoran et al., 2023]. As illustrated in Figure 5, to circumvent these challenges,contemporary research has proposed methods for refining theretrieval process:iterative retrieval, recursive retrieval andadaptive retrieval. Iterative retrieval allows the model to en-gage in multiple retrieval cycles, enhancing the depth andrelevance of the information obtained. Recursive retrievalprocess where the results of one retrieval operation are usedas the input for the subsequent retrieval. It helps to delvedeeper into relevant information, particularly when dealingwith complex or multi-step queries.\\nRecursive retrievalinvolves acquiring smaller chunks during the initial retrievalphase to capture key semantic meanings. Subsequently, largerchunks containing more contextual information are providedto the LLM in later stages of the process. This two-step re-trieval method helps to strike a balance between efficiencyand the delivery of contextually rich responses. StepBack-prompt approach encourages the LLM to moveaway from specific instances and engage in reasoning aroundbroader concepts and principles [Zheng et al., 2023].',\n",
       " 'Knowledgpt: Enhancing large lan-guage models with retrieval and storage access on knowl-edge bases. arXiv preprint arXiv:2308.11761, 2023. [Wang et al., 2023e] Yile Wang, Peng Li, Maosong Sun,Self-knowledge guided retrieval aug-arXiv preprintand Yang Liu.mentation for large language models. arXiv:2310.05002, 2023. [Xia et al., 2019] Mengzhou Xia, Guoping Huang, LemaoLiu, and Shuming Shi.\\n[Es et al., 2023] Shahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. Ragas: Automated eval-uation of retrieval augmented generation. arXiv preprintarXiv:2309.15217, 2023. [Feng et al., 2023] Zhangyin Feng, Xiaocheng Feng, DezhiZhao, Maojin Yang, and Bing Qin. Retrieval-generationsynergy augmented large language models. arXiv preprintarXiv:2310.05149, 2023.',\n",
       " 'Robust retrieval augmented generation for zero-shot slotfilling. arXiv preprint arXiv:2108.13934, 2021. [Google, 2023] Google. Gemini: A family of highly capablemultimodal models. https://goo.gle/GeminiPaper, 2023. [Guo et al., 2023] Zhicheng Guo, Sijie Cheng, Yile Wang,Peng Li, and Yang Liu. Prompt-guided retrieval augmen-tation for non-knowledge-intensive tasks.\\nUprise: Uni-versal prompt retrieval for improving zero-shot evaluation. arXiv preprint arXiv:2303.08518, 2023. [Cheng et al., 2023b] Xin Cheng, Di Luo, Xiuying Chen,Lemao Liu, Dongyan Zhao, and Rui Yan. Lift yourselfup: Retrieval-augmented text generation with self mem-ory. arXiv preprint arXiv:2305.02437, 2023. [Cohere, 2023] Cohere.',\n",
       " 'The RAG system optimizes itsperformance by intelligently integrating various techniques,including keyword-based search, semantic search, and vec-tor search. This approach leverages the unique strengths ofeach method to accommodate diverse query types and infor-mation needs, ensuring consistent retrieval of highly relevantand context-rich information. The use of hybrid search servesas a robust supplement to retrieval strategies, thereby enhanc-ing the overall efficacy of the RAG pipeline. Recursive Retrieval and Query Engine. Recursive retrievalinvolves acquiring smaller chunks during the initial retrievalphase to capture key semantic meanings.\\nAdap-tive retrieval, on the other hand, offers a dynamic adjustmentmechanism, tailoring the retrieval process to the specific de-mands of varying tasks and contexts. Iterative RetrievalIterative retrieval in RAG models is a process where doc-uments are repeatedly collected based on the initial queryand the text generated thus far, providing a more compre-hensive knowledge base for LLMs [Borgeaud et al., 2022,Arora et al., 2023]. This approach has been shown to en-hance the robustness of subsequent answer generation by of-fering additional contextual references through multiple re-trieval iterations.',\n",
       " 'Counterfactual Robustness tests the model’s ability to rec-ognize and disregard known inaccuracies within documents,even when instructed about potential misinformation. Context relevance and noise robustness are important forevaluating the quality of retrieval, while answer faithfulness,answer relevance, negative rejection, information integration,and counterfactual robustness are important for evaluating theThe specific metrics for each evaluation aspect are summa-rized in Table 2. It is essential to recognize that these metrics,derived from related work, are traditional measures and donot yet represent a mature or standardized approach for quan-tifying RAG evaluation aspects.\\nTable 2: Summary of metrics applicable for evaluation aspects of RAGContextRelevanceFaithfulnessAnswerRelevanceNoiseRobustnessNegativeRejectionInformationIntegrationCounterfactualRobustness✓✓✓✓✓✓✓✓✓✓AccuracyEMRecallPrecisionR-RateCosine SimilarityHit RateMRRNDCG✓✓✓✓✓✓Table 3: Summary of evaluation frameworksEvaluation FrameworkEvaluation TargetsEvaluation AspectsQuantitative MetricsRGB†RECALL†RAGAS‡ARES‡TruLens‡Retrieval QualityGeneration QualityNoise Robustness Negative RejectionInformation IntegrationCounterfactual RobustnessAccuracyEMAccuracyAccuracyGeneration Quality Counterfactual Robustness R-Rate (Reappearance Rate)Retrieval QualityGeneration QualityRetrieval QualityGeneration QualityRetrieval QualityGeneration QualityContext RelevanceFaithfulnessAnswer RelevanceContext RelevanceFaithfulnessAnswer RelevanceContext RelevanceFaithfulnessAnswer Relevance**Cosine SimilarityAccuracyAccuracyAccuracy***† represents a benchmark, and ‡ represents a tool.',\n",
       " 'Techniques such as prompt engi-neering, Fine-Tuning (FT), and RAG each have distinct char-acteristics, visually represented in Figure 6. While promptengineering leverages a model’s inherent capabilities, opti-mizing LLMs often requires the application of both RAG andFT methods. The choice between RAG and FT should bebased on the specific requirements of the scenario and the in-herent properties of each approach. A detailed comparison ofRAG and FT is presented in Table 1.6.4 RAG vs Fine-TuningRAG is like giving a model a textbook for tailored informa-tion retrieval, perfect for specific queries.\\nDespite its advantages, fine-tuning has limitations, includ-ing the need for specialized datasets for RAG fine-tuningand the requirement for significant computational resources. However, this stage allows for customizing models to specificneeds and data formats, potentially reducing resource usagecompared to the pre-training phase while still being able tofine-tune the model’s output style. In summary, the fine-tuning stage is essential for the adap-tation of RAG models to specific tasks, enabling the refine-ment of both retrievers and generators.',\n",
       " 'For instance, Flowise AI10 prioritizes alow-code approach, enabling users to deploy AI applications,including RAG, through a user-friendly drag-and-drop inter-face. Other technologies like HayStack, Meltano11, and Co-here Coral12 are also gaining attention for their unique con-tributions to the field. In addition to AI-focused providers, traditional softwareand cloud service providers are expanding their offerings toinclude RAG-centric services. Verba13 from Weaviate is de-signed for personal assistant applications, while Amazon’sKendra14 provides an intelligent enterprise search service, al-lowing users to navigate through various content repositoriesusing built-in connectors.\\nVerba13 from Weaviate is de-signed for personal assistant applications, while Amazon’sKendra14 provides an intelligent enterprise search service, al-lowing users to navigate through various content repositoriesusing built-in connectors. During the evolution of the RAGtechnology landscape, there has been a clear divergence to-wards different specializations, such as: 1) Customization. Tailoring RAG to meet a specific requirements. 2) Simpli-fication. Making RAG easier to use, thereby reducing the ini-tial learning curve. 3) Specialization. Refining RAG to serveproduction environments more effectively.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_build_tools.cluster_search_engine(document_id = 'ee6977e5-854d-44b8-ae90-d52d64808b5d',cluster_number=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "Retrieval-Augmented Generation for Large Language Models: A Survey provides an in-depth overview of the progress in integrating Retrieval-Augmented Generation (RAG) with Large Language Models (LLMs). This comprehensive analysis covers the evolution of RAG technologies, their implementation strategies, and the frameworks developed for enhancing LLMs' performance in knowledge-intensive tasks. Through a detailed examination of Naive, Advanced, and Modular RAG paradigms, the survey underscores innovations that have led to significant advancements in RAG's effectiveness and adaptability. Moreover, it discusses the challenges and future prospects of RAG in broadening its application scope into multimodal domains and improving evaluation methodologies.\n",
    "\n",
    "Main Points\n",
    "Retrieval-Augmented Generation (RAG) has emerged as a key solution for LLMs' challenges, improving their accuracy and relevance by integrating external knowledge sources.\n",
    "Evolution of RAG: The development of RAG technology has progressed through three paradigms—Naive RAG, Advanced RAG, and Modular RAG, each offering improvements in information retrieval, augmentation, and generation processes.\n",
    "Integration with AI Methodologies: RAG's capability is enhanced by incorporating it with other AI approaches like fine-tuning and reinforcement learning, leading to hybrid methods that leverage both parameterized knowledge and extensive non-parameterized data.\n",
    "Challenges: Despite significant advancements, RAG faces challenges such as managing extended contexts, improving robustness, and optimizing retrieval strategies for dynamic applications.\n",
    "Future Prospects: The survey identifies potential areas for further research and development, including scaling laws applicability to RAG, expanding into multimodal data processing, and refining RAG's evaluation frameworks.\n",
    "Impact on AI and Machine Learning: The advancements in RAG technologies hold promise for a wide range of applications, making it a subject of interest for both academic research and industrial innovation.\n",
    "This survey outlines the current landscape, challenges, and potential future directions for RAG research, emphasizing the technology's critical role in extending the capabilities of LLMs through the strategic integration of external knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/codebox39/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple cherche à acheter une start-up anglaise pour 1 milliard de dollars\n",
      "Apple NOUN ROOT\n",
      "cherche NOUN flat:name\n",
      "à ADP mark\n",
      "acheter VERB acl\n",
      "une DET det\n",
      "start NOUN obj\n",
      "- ADJ obj\n",
      "up ADJ det\n",
      "anglaise NOUN obj\n",
      "pour ADP case\n",
      "1 NUM nummod\n",
      "milliard NOUN nmod\n",
      "de ADP case\n",
      "dollars NOUN nmod\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences \n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "doc = nlp(sentences[0])\n",
    "print(doc.text)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
