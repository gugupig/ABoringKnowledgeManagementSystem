{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DocumentProcessing.pdf_processing import pdf_processor\n",
    "from DocumentProcessing.pdf_processing import paper_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_paper = \"/root/gpt_projects/ABoringKnowledgeManagementSystem/DocumentProcessing/tests/1810.04805.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text, title,publish_display_data, authors, abstract, references = paper_processor.extract_text_from_pdf(pdf_paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = pdf_processor.extract_text(pdf_paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_processor.extract_toc_from_pdf(pdf_paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text_2[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_processor.extract_pdf_metadata(pdf_paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_processor.py\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path, max_non_text_pages=10):\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        text = ''\n",
    "        non_text_page_count = 0\n",
    "\n",
    "        for page in doc:\n",
    "            page_text = page.get_text()\n",
    "            if not page_text:\n",
    "                non_text_page_count += 1\n",
    "                if non_text_page_count >= max_non_text_pages:\n",
    "                    raise ValueError(\"PDF contains too many non-text pages. Please use your own OCR tool first.\")\n",
    "                continue\n",
    "            else:\n",
    "                non_text_page_count = 0  # Reset counter if a text page is found\n",
    "            text += page_text\n",
    "\n",
    "        return text\n",
    "\n",
    "def extract_toc_from_pdf(pdf_path, max_non_text_pages=10):\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        toc = doc.get_toc(simple=False)\n",
    "        non_text_page_count = 0\n",
    "\n",
    "        # Check for non-text pages in the document\n",
    "        for page in doc:\n",
    "            if not page.get_text():\n",
    "                non_text_page_count += 1\n",
    "                if non_text_page_count >= max_non_text_pages:\n",
    "                    raise ValueError(\"PDF contains too many non-text pages. Please use your own OCR tool first.\")\n",
    "                continue\n",
    "            else:\n",
    "                non_text_page_count = 0  # Reset counter if a text page is found\n",
    "                break  # Break the loop after finding the first text page\n",
    "\n",
    "        return toc\n",
    "\n",
    "\n",
    "def extract_notes_from_pdf(pdf_path):\n",
    "    notes = []\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for page in doc:\n",
    "            annotations = page.annots()\n",
    "            if annotations:\n",
    "                for annot in annotations:\n",
    "                    annot_text = annot.info['content']\n",
    "                    if annot_text:\n",
    "                        notes.append(annot_text)\n",
    "    return notes\n",
    "\n",
    "\n",
    "def extract_pdf_metadata(pdf_path):\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        metadata = doc.metadata\n",
    "    return metadata\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    pass\n",
    "\n",
    "\n",
    "def extract_pdf_metadata(file_path):\n",
    "    \"\"\"\n",
    "    Extracts metadata from a PDF file.\n",
    "    Args:\n",
    "    file_path (str): Path to the PDF file.\n",
    "    Returns:\n",
    "    dict: Metadata of the PDF.\n",
    "    \"\"\"\n",
    "    metadata = {}\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        metadata = pdf.metadata\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DocumentManagement.Elastic import IndexSettingsGenerator,search_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch, helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(\n",
    "    ['https://localhost:9200'],\n",
    "    http_auth=('elastic', 'dR8dVIqQ5=i3pPSH00zC'),  # Replace with your credentials\n",
    "    verify_certs=True,\n",
    "    ca_certs='/root/http_ca.crt'  # Path to your CA certificate\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tika import parser\n",
    "\n",
    "parsed = parser.from_file(pdf_paper)\n",
    "print(parsed[\"metadata\"])  # To print the metadata of the document.\n",
    "print(parsed[\"content\"])   # To print the text content of the document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tika import parser\n",
    "import re\n",
    "\n",
    "def extract_text_tika(file_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file using Apache Tika, page by page, and checks for consecutive pages without text.\n",
    "    Args:\n",
    "        file_path (str): Path to the PDF file.\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are page numbers and values are extracted text from each page.\n",
    "    Raises:\n",
    "        ValueError: If 10 consecutive pages are found without text.\n",
    "    \"\"\"\n",
    "    # Parse the PDF file\n",
    "    parsed = parser.from_file(file_path)\n",
    "\n",
    "    # Extract the content\n",
    "    content = parsed['content']\n",
    "    if not content:\n",
    "        raise ValueError(\"No content found in the PDF file.\")\n",
    "\n",
    "    # Split the content into pages\n",
    "    pages = content.split('\\f')  # Splitting by form feed character which typically represents page breaks\n",
    "    text = {}\n",
    "    consecutive_pages_without_text = 0\n",
    "\n",
    "    for i, page_content in enumerate(pages, 1):\n",
    "        # Clean up whitespace\n",
    "        page_text = ' '.join(page_content.strip().split())\n",
    "\n",
    "        if not page_text:\n",
    "            consecutive_pages_without_text += 1\n",
    "            if consecutive_pages_without_text == 10:\n",
    "                raise ValueError(\"PDF seems to contain only images. Please use OCR first.\")\n",
    "        else:\n",
    "            consecutive_pages_without_text = 0\n",
    "\n",
    "        text[i] = page_text\n",
    "\n",
    "    return text\n",
    "\n",
    "# Usage\n",
    "# file_path = 'path_to_your_pdf_file.pdf'\n",
    "# pdf_text = extract_text_tika(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tika import parser\n",
    "import re\n",
    "\n",
    "def extract_text_tika(file_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file using Apache Tika, page by page, and checks for consecutive pages without text.\n",
    "    Args:\n",
    "        file_path (str): Path to the PDF file.\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are page numbers and values are extracted text from each page.\n",
    "    Raises:\n",
    "        ValueError: If 10 consecutive pages are found without text.\n",
    "    \"\"\"\n",
    "    # Parse the PDF file\n",
    "    parsed = parser.from_file(file_path)\n",
    "\n",
    "    # Extract the content\n",
    "    content = parsed['content']\n",
    "    if not content:\n",
    "        raise ValueError(\"No content found in the PDF file.\")\n",
    "\n",
    "    # Split the content into pages\n",
    "    pages = content.split('\\f')  # Splitting by form feed character which typically represents page breaks\n",
    "    text = {}\n",
    "    consecutive_pages_without_text = 0\n",
    "\n",
    "    for i, page_content in enumerate(pages, 1):\n",
    "        # Clean up whitespace\n",
    "        page_text = ' '.join(page_content.strip().split())\n",
    "\n",
    "        if not page_text:\n",
    "            consecutive_pages_without_text += 1\n",
    "            if consecutive_pages_without_text == 10:\n",
    "                raise ValueError(\"PDF seems to contain only images. Please use OCR first.\")\n",
    "        else:\n",
    "            consecutive_pages_without_text = 0\n",
    "\n",
    "        text[i] = page_text\n",
    "\n",
    "    return text\n",
    "\n",
    "# Usage\n",
    "# file_path = 'path_to_your_pdf_file.pdf'\n",
    "# pdf_text = extract_text_tika(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tika import parser\n",
    "import re\n",
    "\n",
    "def extract_text_tika(file_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file using Apache Tika, page by page, and checks for consecutive pages without text.\n",
    "    Args:\n",
    "        file_path (str): Path to the PDF file.\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are page numbers and values are extracted text from each page.\n",
    "    Raises:\n",
    "        ValueError: If 10 consecutive pages are found without text.\n",
    "    \"\"\"\n",
    "    # Parse the PDF file\n",
    "    parsed = parser.from_file(file_path)\n",
    "    content = parsed['content']\n",
    "\n",
    "    # Split the content by pages - Tika's plain text output may not provide clear page delimiters\n",
    "    # Adjust the splitting logic based on the actual output format\n",
    "    pages = content.split('some_page_delimiter')  # Replace 'some_page_delimiter' with the actual delimiter used by Tika\n",
    "\n",
    "    text = {}\n",
    "    consecutive_pages_without_text = 0\n",
    "\n",
    "    for i, page_content in enumerate(pages, start=1):\n",
    "        page_text = page_content.strip()\n",
    "\n",
    "        if not page_text:\n",
    "            consecutive_pages_without_text += 1\n",
    "            if consecutive_pages_without_text == 10:\n",
    "                raise ValueError(\"10 consecutive pages without text found. PDF may contain only images. Please use OCR.\")\n",
    "        else:\n",
    "            consecutive_pages_without_text = 0\n",
    "\n",
    "        text[i] = page_text\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_text = extract_text_tika(pdf_paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from io import BytesIO\n",
    "\n",
    "def add_invisible_markers(pdf_path, marker):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    writer = PdfWriter()\n",
    "\n",
    "    for i in range(len(reader.pages)):\n",
    "        page = reader.pages[i]\n",
    "        packet = BytesIO()\n",
    "        can = canvas.Canvas(packet, pagesize=letter)\n",
    "        can.setFontSize(1)  # Setting font size to 1 or 0 to make it nearly invisible\n",
    "        can.drawString(0, 0, marker)  # Position at bottom-left; can adjust as needed\n",
    "        can.save()\n",
    "\n",
    "        packet.seek(0)\n",
    "        new_pdf = PdfReader(packet)\n",
    "        page.merge_page(new_pdf.pages[0])\n",
    "        writer.add_page(page)\n",
    "\n",
    "    with open(\"/root/gpt_projects/ABoringKnowledgeManagementSystem/DocumentProcessing/tests/modified_pdf.pdf\", \"wb\") as f:\n",
    "        writer.write(f)\n",
    "\n",
    "add_invisible_markers(pdf_paper ,\"__PageBreak__12345__\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tika import parser\n",
    "\n",
    "def extract_pages_with_tika(file_path, marker):\n",
    "    parsed = parser.from_file(file_path)\n",
    "    content = parsed['content']\n",
    "    pages = content.split(marker)  # Splitting text using the marker\n",
    "\n",
    "    text_by_page = {i+1: page.strip() for i, page in enumerate(pages)}\n",
    "    return text_by_page\n",
    "\n",
    "text_by_page = extract_pages_with_tika(\"/root/gpt_projects/ABoringKnowledgeManagementSystem/DocumentProcessing/tests/modified_pdf.pdf\", \"__PageBreak__12345__\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_by_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = pdf_processor.extract_text(pdf_paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_by_page[1], full_text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DocumentIndexing.MongoDB import documentstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = documentstore.get_document_from_collection('research_paper', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "PAGE = \"PDF VIEWER\"\n",
    "pickle.dump( PAGE, open(CHAT_BOT_STATUT_CACHE_PATH, \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (CHAT_BOT_STATUT_CACHE_PATH, 'rb') as fp:\n",
    "    itemlist = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_BOT_STATUT_CACHE_PATH = \"/root/gpt_projects/ABoringKnowledgeManagementSystem/WebApp/WebUI/static/chat_bot_statut_cache/chat_bot_statut_cache.pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_VIEWER_CACHE_PATH = \"/root/gpt_projects/ABoringKnowledgeManagementSystem/WebApp/WebUI/static/pdf_viewer_cache/pdf_viewer_cache.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_FILE = ('d42a0cd9-fdfd-4639-a38c-a85b08cd37a7','research_paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( PDF_FILE, open(PDF_VIEWER_CACHE_PATH, \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DocumentIndexing.Elastic import IndexSettingsGenerator\n",
    "from DocumentIndexing.Elastic import search_engine\n",
    "from elasticsearch.helpers import scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/codebox39/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from DocumentIndexing.Embedding.embedding import TextEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DocumentIndexing.Elastic.search_engine import SearchEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_engine = SearchEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_engine.search_for_terms('research_paper_chunk_level','bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Chat import rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = rag.two_step_retrival_page(\"what is BERT?\",\"2610a7e7-359f-4e79-85d8-d3e3dff51dab\", \"research_paper\",return_top_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rag.two_step_prompt_page(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = rag.one_document_prompt(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexsetting = IndexSettingsGenerator.GeneralIndexSettings(EMBEDDING_DINENSION)\n",
    "chunk_mapping = indexsetting.chunk_level_mapping()\n",
    "document_mapping = indexsetting.document_level_mapping()\n",
    "page_mapping = indexsetting.page_level_mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_191312/2026850524.py:2: DeprecationWarning: The 'http_auth' parameter is deprecated. Use 'basic_auth' or 'bearer_auth' parameters instead\n",
      "  es = Elasticsearch(hosts=ES_HOST, http_auth=ES_HTTP_AUTH, verify_certs=True, ca_certs=ES_CA_CERTS_PATH)\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch(hosts=ES_HOST, http_auth=ES_HTTP_AUTH, verify_certs=True, ca_certs=ES_CA_CERTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mappings': {'properties': {'document_id_elastic': {'type': 'keyword'}, 'document_id_universal': {'type': 'keyword'}, 'page_number': {'type': 'integer'}, 'chunk_number': {'type': 'integer'}, 'upload_date': {'type': 'date', 'format': 'epoch_millis'}, 'max_split_number': {'type': 'integer'}, 'text_piece': {'type': 'text'}, 'language': {'type': 'keyword'}, 'text_piece_vector': {'type': 'dense_vector', 'dims': 512}, 'metadata': {'type': 'object', 'dynamic': True}, 'document_tags': {'type': 'keyword'}, 'related_documents': {'type': 'object', 'dynamic': True}, 'acheived': {'type': 'boolean'}}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_191312/1163721322.py:4: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  response = es.indices.delete(index=f'research_paper_'+level, ignore=[400, 404])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mappings': {'properties': {'document_id_elastic': {'type': 'keyword'}, 'document_id_universal': {'type': 'keyword'}, 'page_number': {'type': 'integer'}, 'upload_date': {'type': 'date', 'format': 'epoch_millis'}, 'max_split_number': {'type': 'integer'}, 'text_piece': {'type': 'text'}, 'language': {'type': 'keyword'}, 'page_summary': {'type': 'text'}, 'page_smmary_vector': {'type': 'dense_vector', 'dims': 512}, 'text_piece_vector': {'type': 'dense_vector', 'dims': 512}, 'metadata': {'type': 'object', 'dynamic': True}, 'document_tags': {'type': 'keyword'}, 'related_documents': {'type': 'object', 'dynamic': True}, 'acheived': {'type': 'boolean'}}}}\n",
      "{'mappings': {'properties': {'document_id_elastic': {'type': 'keyword'}, 'document_id_universal': {'type': 'keyword'}, 'upload_date': {'type': 'date', 'format': 'epoch_millis'}, 'max_page_number': {'type': 'integer'}, 'document_title': {'type': 'keyword'}, 'document_summary': {'type': 'text'}, 'language': {'type': 'keyword'}, 'document_title_vector': {'type': 'dense_vector', 'dims': 512}, 'document_summary_vector': {'type': 'dense_vector', 'dims': 512}, 'text_piece_vector': {'type': 'dense_vector', 'dims': 512}, 'metadata': {'type': 'object', 'dynamic': True}, 'document_tags': {'type': 'keyword'}, 'related_documents': {'type': 'object', 'dynamic': True}, 'acheived': {'type': 'boolean'}}}}\n"
     ]
    }
   ],
   "source": [
    "new_search_engine = search_engine.SearchEngine()    \n",
    "for mapping,level in zip([chunk_mapping, page_mapping,document_mapping],DOCUMENT_LEVEL):\n",
    "    print(mapping)\n",
    "    response = es.indices.delete(index=f'research_paper_'+level, ignore=[400, 404])\n",
    "    new_search_engine.create_index(index_name=f'research_paper_'+level, index_settings=mapping)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eleasticengine = search_engine.SearchEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index_name in DOCUMENT_TYPE:\n",
    "    eleasticengine.create_index(index_name,mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"research_paper_chunk_level\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import scan\n",
    "\n",
    "# Connect to Elasticsearch\n",
    "es = Elasticsearch(hosts=ES_HOST, http_auth=ES_HTTP_AUTH, verify_certs=True, ca_certs=ES_CA_CERTS_PATH)\n",
    "# Index name\n",
    "index_name = \"research_paper\"\n",
    "\n",
    "# Initialize the scan\n",
    "results = scan(es, index=index_name, query={\"query\": {\"match_all\": {}}})\n",
    "\n",
    "# Iterate over the results\n",
    "for doc in results:\n",
    "    print(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import DOCUMENT_TYPE, ES_HOST, ES_HTTP_AUTH, ES_CA_CERTS_PATH, EMBEDDING_DINENSION,DOCUMENT_LEVEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch(hosts=ES_HOST, http_auth=ES_HTTP_AUTH, verify_certs=True, ca_certs=ES_CA_CERTS_PATH)\n",
    "\n",
    "\n",
    "existing_indices = DOCUMENT_TYPE\n",
    "# Delete each index\n",
    "for index in existing_indices:\n",
    "    response = es.indices.delete(index=index)\n",
    "    print(f\"Deleted index: {index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['hits']['hits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = embedding_local.embeddings_multilingual(\"ﬁnetuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = eleasticengine.vector_search(\"research_paper\",vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2['hits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdftitle\n",
    "print(pdftitle.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdftitle import get_title_from_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = '/root/gpt_projects/ABoringKnowledgeManagementSystem/DocumentBank/research_paper/416f1ec4-bcbd-4204-9069-b6ea86c1b7a9.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    title = get_title_from_file(fp)\n",
    "    print(f\"The extracted title is: {title}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'DocumentBank/research_paper/2610a7e7-359f-4e79-85d8-d3e3dff51dab.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp= \"/root/downloads/MY_CV_Final.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DocumentManagement.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Document(file_path = 'DocumentBank/research_paper/2610a7e7-359f-4e79-85d8-d3e3dff51dab.pdf',document_type='research_paper',document_id='123')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DocumentProcessing.pdf_processing import pdf_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_processor.extract_pdf_metadata(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_processor.structured_metadata_for_paper(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from llama_hub.file.unstructured import UnstructuredReader\n",
    "\n",
    "loader = UnstructuredReader()\n",
    "documents = loader.load_data(file=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index.ingestion import IngestionPipeline\n",
    "from llama_index.node_parser import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = IngestionPipeline(transformations=[TokenTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=20,\n",
    "    separator=\" \",\n",
    ")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.text_splitter import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = pipeline.run(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.text_splitter import TokenTextSplitter\n",
    "\n",
    "splitter = TokenTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=20,\n",
    "    separator=\" \",\n",
    ")\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex(nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "es = Elasticsearch(hosts=ES_HOST, http_auth=ES_HTTP_AUTH, verify_certs=True, ca_certs=ES_CA_CERTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.elasticsearch import ElasticsearchEmbedding\n",
    "from llama_index.vector_stores import ElasticsearchStore\n",
    "from llama_index import ServiceContext, StorageContext, VectorStoreIndex\n",
    "\n",
    "\n",
    "\n",
    "from llama_index.vector_stores import ElasticsearchStore\n",
    "\n",
    "vector_store = ElasticsearchStore(\n",
    "    index_name= index_name,\n",
    "    es_client=es,     \n",
    "    vwctor_field=\"text_piece_vector\",  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'We also undertake systematic study of “data contamination” – growing problem when training high capacity models\\non datasets such as Common Crawl which can potentially include content from test datasets simply because such\\ncontent often exists on the web In this paper we develop systematic tools to measure data contamination and quantify\\nits distorting effects Although we ﬁnd that data contamination has minimal effect on GPT’ performance on most\\ndatasets we do identify few datasets where it could be inﬂating results and we either do not report results on these\\ndatasets or we note them with an asterisk depending on the severity\\nIn addition to all the above we also train series of smaller models ranging from 125 million parameters to 13 billion\\nparameters in order to compare their performance to GPT in the zero one and fewshot settings Broadly for most\\ntasks we ﬁnd relatively smooth scaling with model capacity in all three settings one notable pattern is that the gap\\nbetween zero one and fewshot performance often grows with model capacity perhaps suggesting that larger models\\nare more proﬁcient metalearners\\nFinally given the broad spectrum of capabilities displayed by GPT we discuss concerns about bias fairness and\\nbroader societal impacts and attempt preliminary analysis of GPT’ characteristics in this regard\\nThe remainder of this paper is organized as follows In Section we describe our approach and methods for training\\nGPT and evaluating it Section presents results on the full range of tasks in the zero one and fewshot settings\\nSection addresses questions of data contamination traintest overlap Section discusses limitations of GPT\\nSection discusses broader impacts Section reviews related work and Section concludes\\n Approach\\nOur basic pretraining approach including model data and training is similar to the process described in RWC19\\nwith relatively straightforward scaling up of the model size dataset size and diversity and length of training Our use\\nof incontext learning is also similar to RWC19 but in this work we systematically explore different settings for\\nlearning within the context Therefore we start this section by explicitly deﬁning and contrasting the different settings\\nthat we will be evaluating GPT on or could in principle evaluate GPT on These settings can be seen as lying on \\nspectrum of how much taskspeciﬁc data they tend to rely on Speciﬁcally we can identify at least four points on this\\nspectrum see Figure for an illustration\\n• FineTuning FT has been the most common approach in recent years and involves updating the weights of\\n pretrained model by training on supervised dataset speciﬁc to the desired task Typically thousands to\\nhundreds of thousands of labeled examples are used The main advantage of ﬁnetuning is strong performance\\non many benchmarks The main disadvantages are the need for new large dataset for every task the potential\\nfor poor generalization outofdistribution MPL19 and the potential to exploit spurious features of the\\ntraining data GSL18 NK19 potentially resulting in an unfair comparison with human performance In\\nthis work we do not ﬁnetune GPT because our focus is on taskagnostic performance but GPT can be\\nﬁnetuned in principle and this is promising direction for future work\\n• FewShot FS is the term we will use in this work to refer to the setting where the model is given few\\ndemonstrations of the task at inference time as conditioning RWC19 but no weight updates are allowed\\nAs shown in Figure for typical dataset an example has context and desired completion for example\\nan English sentence and the French translation and fewshot works by giving examples of context and\\ncompletion and then one ﬁnal example of context with the model expected to provide the completion We\\ntypically set in the range of 10 to 100 as this is how many examples can ﬁt in the model’ context window\\nnctx 2048 The main advantages of fewshot are major reduction in the need for taskspeciﬁc data and\\nreduced potential to learn an overly narrow distribution from large but narrow ﬁnetuning dataset The main\\ndisadvantage is that results from this method have so far been much worse than stateoftheart ﬁnetuned\\nmodels Also small amount of task speciﬁc data is still required As indicated by the name fewshot\\nlearning as described here for language models is related to fewshot learning as used in other contexts in\\nML HYC01 VBL16 – both involve learning based on broad distribution of tasks in this case implicit in\\nthe pretraining data and then rapidly adapting to new task\\n• OneShot 1S is the same as fewshot except that only one demonstration is allowed in addition to natural\\nlanguage description of the task as shown in Figure The reason to distinguish oneshot from fewshot and\\nzeroshot below is that it most closely matches the way in which some tasks are communicated to humans\\nFor example when asking humans to generate dataset on human worker service for example Mechanical\\nTurk it is common to give one demonstration of the task By contrast it is sometimes difﬁcult to communicate\\nthe content or format of task if no examples are given'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DocumentIndexing.Embedding import embedding_toolkits\n",
    "from DocumentIndexing.Embedding import text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_toolkits.w2v_token_len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_toolkits.token_length(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter.split_text_with_langchain(text,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('distiluse-base-multilingual-cased-v2')\n",
    "token = model.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(token['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/distiluse-base-multilingual-cased-v2')\n",
    "encoded_input = tokenizer.encode_plus(text, add_special_tokens=True, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = encoded_input['input_ids'][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([token for token in input_ids if token not in tokenizer.all_special_ids])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0, model_name=\"sentence-transformers/distiluse-base-multilingual-cased-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter.maximum_tokens_per_chunk = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter.count_tokens(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/distiluse-base-multilingual-cased-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split= splitter.split_text(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter.count_tokens(text=split[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load your SentenceTransformer model\n",
    "model_name = \"sentence-transformers/distiluse-base-multilingual-cased-v2\"  # Replace with your model name\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Access the underlying Hugging Face model\n",
    "hf_model = model._first_module().auto_model\n",
    "\n",
    "# Check the maximum input length\n",
    "max_length = hf_model.config.max_position_embeddings\n",
    "print(\"Maximum input token limit:\", max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacypdfreader.spacypdfreader import pdf_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = pdf_reader('/root/gpt_projects/ABoringKnowledgeManagementSystem/DocumentBank/research_paper/06063c65-7cc9-4065-8c26-03d8d7aa6379.pdf', nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc._.page(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = []\n",
    "for i in range (1,2):\n",
    "    for token in doc._.page(i):\n",
    "        if token.text.endswith('\\n'):\n",
    "        # Remove the line break and do not add a space\n",
    "            new_text += token.text.rstrip('\\n')\n",
    "        else:\n",
    "            # Add token text with a following space\n",
    "            new_text += token.text_with_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for token in list(doc._.page(1).sents):\n",
    "    if token.text.endswith('\\n'):\n",
    "        # Remove the line break and do not add a space\n",
    "        new_text += token.text.rstrip('\\n')\n",
    "    else:\n",
    "        # Add token text with a following space\n",
    "        new_text += token.text_with_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = list(doc._.page(1).sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents[0].text_with_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer\n",
    "for page_layout in extract_pages('/root/gpt_projects/ABoringKnowledgeManagementSystem/DocumentBank/research_paper/06063c65-7cc9-4065-8c26-03d8d7aa6379.pdf'):\n",
    "    for element in page_layout:\n",
    "        if isinstance(element, LTTextContainer):\n",
    "            print(element.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DocumentProcessing.pdf_processing import pdf_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor  = pdf_processor.PDFProcessor('/root/gpt_projects/ABoringKnowledgeManagementSystem/DocumentBank/research_paper/06063c65-7cc9-4065-8c26-03d8d7aa6379.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = processor.extract_text_from_pdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DocumentIndexing.Embedding import text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spliter = text_splitter.TextSplitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spliter.split_text_with_langchain_recursive(pages[1],128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spliter.split_text_with_langchain_sentence_transformers(pages[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DocumentIndexing.Embedding.embedding_toolkits import token_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_sentences(sentences, max_tokens=128):\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_token_count = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Tokenize the sentence and count the tokens\n",
    "        token_count = token_length(sentence.text)\n",
    "\n",
    "        # If adding this sentence exceeds the max token count, start a new chunk\n",
    "        if current_token_count + token_count > max_tokens:\n",
    "            if current_chunk:  # Ensure current chunk is not empty\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence.text]\n",
    "            current_token_count = token_count\n",
    "        else:\n",
    "            # Add the sentence to the current chunk\n",
    "            current_chunk.append(sentence.text)\n",
    "            current_token_count += token_count\n",
    "\n",
    "    # Add the last chunk if it's not empty\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_sentences(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[chunks.replace('\\n', '') for chunks in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_sentences_with_sentence_overlap(sentences, max_tokens=128, overlap=0):\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_token_count = 0\n",
    "    overlap_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        token_count = token_length(sentence)\n",
    "\n",
    "        if current_token_count + token_count > max_tokens:\n",
    "            if current_chunk:  # Ensure current chunk is not empty\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "            # Start new chunk with overlap from previous chunk\n",
    "            current_chunk = overlap_sentences + [sentence]\n",
    "            current_token_count = len(' '.join(current_chunk).split())\n",
    "            # Update overlap sentences\n",
    "            overlap_sentences = current_chunk[-overlap:] if len(current_chunk) > overlap else current_chunk[:]\n",
    "        else:\n",
    "            # Add the sentence to the current chunk\n",
    "            current_chunk.append(sentence)\n",
    "            current_token_count += token_count\n",
    "            # Update overlap sentences\n",
    "            if len(current_chunk) > overlap:\n",
    "                overlap_sentences = current_chunk[-overlap:]\n",
    "\n",
    "    # Add the last chunk if it's not empty\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_o = chunk_sentences(sents, overlap=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[chunks.replace('\\n', '') for chunks in chunks_o]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_sentences_with_sentence_overlap_generator(sentences, max_tokens=128, overlap=0):\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_token_count = 0\n",
    "    overlap_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        token_count = token_length(sentence.text)\n",
    "\n",
    "        if current_token_count + token_count > max_tokens:\n",
    "            if current_chunk:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "            # Start new chunk with overlap from previous chunk\n",
    "            current_chunk = overlap_sentences + [sentence.text]\n",
    "            current_token_count = len(' '.join(current_chunk).split())\n",
    "            # Prepare overlap sentences for next chunk\n",
    "            overlap_sentences = current_chunk[-overlap:] if len(current_chunk) > overlap else current_chunk[:]\n",
    "        else:\n",
    "            current_chunk.append(sentence.text)\n",
    "            current_token_count += token_count\n",
    "            if len(current_chunk) > overlap:\n",
    "                overlap_sentences = current_chunk[-overlap:]\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_g = chunk_sentences_with_sentence_overlap_generator(doc._.page(1).sents, overlap=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[chunks.replace('\\n', '') for chunks in chunks_g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/codebox39/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from DocumentIndexing.Embedding.text_splitter import TextSplitter_Spacy\n",
    "from DocumentProcessing.pdf_processing.pdf_processor import SpacyPdfProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "spacy_processor = SpacyPdfProcessor('/root/gpt_projects/ABoringKnowledgeManagementSystem/WebApp/d42a0cd9-fdfd-4639-a38c-a85b08cd37a7.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = spacy_processor.extract_text_from_pdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spliter = TextSplitter_Spacy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = spliter.chunk_sentences_with_sentence_overlap(pages[1], 128,overlap=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BERT: Pre-training of Deep Bidirectional Transformers forLanguage UnderstandingJacob Devlin Ming-Wei Chang Kenton Lee Kristina ToutanovaGoogle AI Language{jacobdevlin,mingweichang,kentonl,kristout}@google.com9102 yaM 42  ]LC.sc [  2v50840.',\n",
       " '[  2v50840. 0181:viXraAbstractWe introduce a new language representa-tion model called BERT, which stands forBidirectional Encoder Representations fromTransformers. Unlike recent language repre-sentation models (Peters et al., 2018a; Rad-ford et al., 2018), BERT is designed to pre-train deep bidirectional representations fromunlabeled text by jointly conditioning on bothleft and right context in all layers.',\n",
       " 'Unlike recent language repre-sentation models (Peters et al., 2018a; Rad-ford et al., 2018), BERT is designed to pre-train deep bidirectional representations fromunlabeled text by jointly conditioning on bothleft and right context in all layers. As a re-sult, the pre-trained BERT model can be ﬁne-tuned with just one additional output layerto create state-of-the-art models for a widerange of tasks, such as question answering andlanguage inference, without substantial task-speciﬁc architecture modiﬁcations. BERT is conceptually simple and empiricallypowerful.',\n",
       " 'BERT is conceptually simple and empiricallypowerful. It obtains new state-of-the-art re-sults on eleven natural language processingtasks, including pushing the GLUE score to80.5% (7.7% point absolute improvement),MultiNLI accuracy to 86.7% (4.6% absoluteimprovement), SQuAD v1.1 question answer-ing Test F1 to 93.2 (1.5 point absolute im-provement) and SQuAD v2.0 Test F1 to 83.1(5.1 point absolute improvement). 1IntroductionLanguage model pre-training has been shown tobe effective for improving many natural languageprocessing tasks (Dai and Le, 2015; Peters et al.,2018a; Radford et al., 2018; Howard and Ruder,2018).',\n",
       " '1IntroductionLanguage model pre-training has been shown tobe effective for improving many natural languageprocessing tasks (Dai and Le, 2015; Peters et al.,2018a; Radford et al., 2018; Howard and Ruder,2018). These include sentence-level tasks such asnatural language inference (Bowman et al., 2015;Williams et al., 2018) and paraphrasing (Dolanand Brockett, 2005), which aim to predict the re-lationships between sentences by analyzing themholistically, as well as token-level tasks such asnamed entity recognition and question answering,where models are required to produce ﬁne-grainedoutput at the token level (Tjong Kim Sang andDe Meulder, 2003; Rajpurkar et al., 2016).',\n",
       " 'These include sentence-level tasks such asnatural language inference (Bowman et al., 2015;Williams et al., 2018) and paraphrasing (Dolanand Brockett, 2005), which aim to predict the re-lationships between sentences by analyzing themholistically, as well as token-level tasks such asnamed entity recognition and question answering,where models are required to produce ﬁne-grainedoutput at the token level (Tjong Kim Sang andDe Meulder, 2003; Rajpurkar et al., 2016). There are two existing strategies for apply-ing pre-trained language representations to down-stream tasks: feature-based and ﬁne-tuning.',\n",
       " 'There are two existing strategies for apply-ing pre-trained language representations to down-stream tasks: feature-based and ﬁne-tuning. Thefeature-based approach, such as ELMo (Peterset al., 2018a), uses task-speciﬁc architectures thatinclude the pre-trained representations as addi-tional features. The ﬁne-tuning approach, such asthe Generative Pre-trained Transformer (OpenAIGPT) (Radford et al., 2018), introduces minimaltask-speciﬁc parameters, and is trained on thedownstream tasks by simply ﬁne-tuning all pre-trained parameters.',\n",
       " 'The ﬁne-tuning approach, such asthe Generative Pre-trained Transformer (OpenAIGPT) (Radford et al., 2018), introduces minimaltask-speciﬁc parameters, and is trained on thedownstream tasks by simply ﬁne-tuning all pre-trained parameters. The two approaches share thesame objective function during pre-training, wherethey use unidirectional language models to learngeneral language representations. We argue that current techniques restrict thepower of the pre-trained representations, espe-cially for the ﬁne-tuning approaches. The ma-jor limitation is that standard language models areunidirectional, and this limits the choice of archi-tectures that can be used during pre-training.',\n",
       " 'The ma-jor limitation is that standard language models areunidirectional, and this limits the choice of archi-tectures that can be used during pre-training. Forexample, in OpenAI GPT, the authors use a left-to-right architecture, where every token can only at-tend to previous tokens in the self-attention layersof the Transformer (Vaswani et al., 2017). Such re-strictions are sub-optimal for sentence-level tasks,and could be very harmful when applying ﬁne-tuning based approaches to token-level tasks suchas question answering, where it is crucial to incor-porate context from both directions.',\n",
       " 'Such re-strictions are sub-optimal for sentence-level tasks,and could be very harmful when applying ﬁne-tuning based approaches to token-level tasks suchas question answering, where it is crucial to incor-porate context from both directions. In this paper, we improve the ﬁne-tuning basedapproaches by proposing BERT: BidirectionalEncoder Representationsfrom Transformers. BERT alleviates the previously mentioned unidi-rectionality constraint by using a “masked lan-guage model” (MLM) pre-training objective, in-spired by the Cloze task (Taylor, 1953).',\n",
       " 'BERT alleviates the previously mentioned unidi-rectionality constraint by using a “masked lan-guage model” (MLM) pre-training objective, in-spired by the Cloze task (Taylor, 1953). Themasked language model randomly masks some ofthe tokens from the input, and the objective is topredict the original vocabulary id of the masked\\x0c']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_generator = spacy_processor.generate_senteces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(spacy_processor.first,spacy_processor.last+1):\n",
    "    print(spliter.chunk_sentences_with_sentence_overlap(next(page_generator)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_generator = spacy_processor.generate_senteces_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_nb, page = next(page_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = spliter.chunk_sentences_with_sentence_overlap_generator(page, 128,overlap=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " ['Figure 1: Overall pre-training and ﬁne-tuning procedures for BERT. Apart from output layers, the same architec-tures are used in both pre-training and ﬁne-tuning. The same pre-trained model parameters are used to initializemodels for different down-stream tasks. During ﬁne-tuning, all parameters are ﬁne-tuned. [CLS] is a specialsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-tions/answers).',\n",
       "  '[CLS] is a specialsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-tions/answers). ing and auto-encoder objectives have been usedfor pre-training such models (Howard and Ruder,2018; Radford et al., 2018; Dai and Le, 2015). 2.3 Transfer Learning from Supervised DataThere has also been work showing effective trans-fer from supervised tasks with large datasets, suchas natural language inference (Conneau et al.,2017) and machine translation (McCann et al.,2017).',\n",
       "  '2.3 Transfer Learning from Supervised DataThere has also been work showing effective trans-fer from supervised tasks with large datasets, suchas natural language inference (Conneau et al.,2017) and machine translation (McCann et al.,2017). Computer vision research has also demon-strated the importance of transfer learning fromlarge pre-trained models, where an effective recipeis to ﬁne-tune models pre-trained with Ima-geNet (Deng et al., 2009; Yosinski et al., 2014). 3 BERTWe introduce BERT and its detailed implementa-tion in this section. There are two steps in ourframework: pre-training and ﬁne-tuning.',\n",
       "  'There are two steps in ourframework: pre-training and ﬁne-tuning. Dur-ing pre-training, the model is trained on unlabeleddata over different pre-training tasks. For ﬁne-tuning, the BERT model is ﬁrst initialized withthe pre-trained parameters, and all of the param-eters are ﬁne-tuned using labeled data from thedownstream tasks. Each downstream task has sep-arate ﬁne-tuned models, even though they are ini-tialized with the same pre-trained parameters. Thequestion-answering example in Figure 1 will serveas a running example for this section.',\n",
       "  'Thequestion-answering example in Figure 1 will serveas a running example for this section. A distinctive feature of BERT is its uniﬁed ar-chitecture across different tasks. There is mini-mal difference between the pre-trained architec-ture and the ﬁnal downstream architecture.',\n",
       "  'There is mini-mal difference between the pre-trained architec-ture and the ﬁnal downstream architecture. Model Architecture BERT’s model architec-ture is a multi-layer bidirectional Transformer en-coder based on the original implementation de-scribed in Vaswani et al. (2017) and released inthe tensor2tensor library.1 Because the useof Transformers has become common and our im-plementation is almost identical to the original,we will omit an exhaustive background descrip-tion of the model architecture and refer readers toVaswani et al. (2017) as well as excellent guidessuch as “The Annotated Transformer. ”2',\n",
       "  '”2 In this work, we denote the number of layers(i.e., Transformer blocks) as L, the hidden size asH, and the number of self-attention heads as A.3We primarily report results on two model sizes:BERTBASE (L=12, H=768, A=12, Total Param-eters=110M) and BERTLARGE (L=24, H=1024,A=16, Total Parameters=340M). BERTBASE was chosen to have the same modelsize as OpenAI GPT for comparison purposes.',\n",
       "  'BERTBASE was chosen to have the same modelsize as OpenAI GPT for comparison purposes. Critically, however, the BERT Transformer usesbidirectional self-attention, while the GPT Trans-former uses constrained self-attention where everytoken can only attend to context to its left.41https://github.com/tensorﬂow/tensor2tensor2http://nlp.seas.harvard.edu/2018/04/03/attention.html3In all cases we set the feed-forward/ﬁlter size to be 4H,i.e., 3072 for the H = 768 and 4096 for the H = 1024. 4We note that in the literature the bidirectional Trans-BERTBERTE[CLS]E1 E[SEP]...ENE1’...EM’CT1T[SEP]... TNT1’...TM’[CLS]Tok 1',\n",
       "  'TNT1’...TM’[CLS]Tok 1 [SEP]... Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1’...EM’CT1T[SEP]... TNT1’...TM’[CLS]Tok 1 [SEP]...',\n",
       "  '[SEP]... Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\x0c'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_nb,split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DocumentManagement.documents import PDFDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = PDFDocument(file_path = '/root/gpt_projects/ABoringKnowledgeManagementSystem/DocumentBank/research_paper/06063c65-7cc9-4065-8c26-03d8d7aa6379.pdf',document_type='research_paper',document_id='123')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/gpt_projects/ABoringKnowledgeManagementSystem/DocumentManagement/documents.py:37\u001b[0m, in \u001b[0;36mDocument.set_language\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 37\u001b[0m     text_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m500\u001b[39m]\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gpt_projects/ABoringKnowledgeManagementSystem/DocumentManagement/documents.py:59\u001b[0m, in \u001b[0;36mPDFDocument.process_document\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_document\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_language\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     processor \u001b[38;5;241m=\u001b[39m SpacyPdfProcessor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mextract_text_from_pdf(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path)\n",
      "File \u001b[0;32m~/gpt_projects/ABoringKnowledgeManagementSystem/DocumentManagement/documents.py:39\u001b[0m, in \u001b[0;36mDocument.set_language\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     37\u001b[0m     text_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m500\u001b[39m]\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     text_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage \u001b[38;5;241m=\u001b[39m detect_language(text_sample)\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "pdf.process_document()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
